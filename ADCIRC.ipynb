{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show how storm perturbations affect storm surge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read multiple directories containing output from ADCIRC and produced by these NCL scripts\n",
    "<pre>\n",
    "/glade/p/work/ahijevyc/ADCIRC/bulge_timeseries.ncl  \n",
    "/glade/p/work/ahijevyc/ADCIRC/perfect_cntl.ncl\n",
    "</pre>\n",
    "These directories can hold different speed perturbations, veer perturbations, vmax, or rmax perturbations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed plots in web page. No floating window.\n",
    "%matplotlib inline\n",
    "# svg increases resolution when you zoom in (Ctrl-+); png does not.\n",
    "# Use svg format (scalable vector graphics) for plots in web page, not png\n",
    "%config InlineBackend.figure_formats=['png']\n",
    "dpi = 300\n",
    "clean = False # clean plots for journal\n",
    "debug = False\n",
    "timestamp = not clean # \"created by\" timestamp in lower-left corner\n",
    "\n",
    "savfig_dict = {\"dpi\":dpi, \"timestamp\": timestamp} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USAGE\n",
    "### Casper"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cd /glade/work/ahijevyc/ADCIRC\n",
    "source /glade/work/ahijevyc/20190723/bin/activate.csh\n",
    "start-jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import dates\n",
    "from matplotlib.ticker import NullFormatter, NullLocator# for side histogram\n",
    "import glob, sys, os, re, urllib.request\n",
    "from netCDF4 import Dataset, chartostring, num2date\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from metpy.units import units\n",
    "#import cf_units # Provision of wrapper class to support UDUNITS-2, netcdftime calendar\n",
    "\n",
    "#plt.rcParams.update({'font.size': 12})\n",
    "#plt.rcParams.update({'lines.markersize': 8})\n",
    "import cache\n",
    "import pdb\n",
    "def mysavfig(ofile, string=\"\", timestamp=True, size=5, debug=False, **kwargs):\n",
    "    if debug:\n",
    "        print('mysavfig: timestamp=',timestamp)\n",
    "    if timestamp:\n",
    "        string = string + '\\n' + 'created '+str(dt.datetime.now(tz=None)).split('.')[0] # + '\\n' # extra newline to keep timestamp onscreen.\n",
    "    th = plt.annotate(string, xy=(2,1), xycoords='figure pixels', horizontalalignment='left', verticalalignment='bottom', size=size)\n",
    "    #plt.tight_layout() # Tried this but it screwed up SHARPY_skewt. Also tried bbox_inches='tight' below but not in that version of matplotlib.\n",
    "    # Tried this but it made a large blank space on the left side of SHARPY_skewt.\n",
    "    #    plt.savefig(ofile,dpi=dpi,bbox_inches='tight')\n",
    "    plt.savefig(ofile, **kwargs)\n",
    "    th.remove() # permits the figure to reused without overlaying multiple \"created . . \" dates.\n",
    "    cmd = \"mogrify +matte -type Palette -colors 255 \" + ofile # prevents flickering when displaying on yellowstone.\n",
    "    print(\"created\", ofile)\n",
    "    return call(cmd.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "knot"
      ],
      "text/latex": [
       "$\\mathrm{knot}$"
      ],
      "text/plain": [
       "<Unit('knot')>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "units.kts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Define Storm Class and Perturbation Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class storm:\n",
    "    # Each storm is initiated with a name and a domain.\n",
    "    # and a string describing the control run directory name\n",
    "    # and possibly a landfall attribute and landfall2 attribute\n",
    "    \n",
    "    def __init__(self, name, domain, control_str='control'):\n",
    "        self.name = name\n",
    "        # directory containing all the perturbation subdirectories.\n",
    "        self.basedir = '/glade/work/ahijevyc/ADCIRC/'+name+'/'\n",
    "        self.domain = domain\n",
    "        self.control_str = control_str\n",
    "        \n",
    "        if name == 'IKE':\n",
    "            self.landfall = {\"time\": dt.datetime(2008,9,13,7),\n",
    "                            \"vmax\":  95. * units.kts,\n",
    "                            \"mslp\": 950. * units.mb\n",
    "                            }\n",
    "        if name == 'CHARLEY':\n",
    "            self.landfall = {\"time\": dt.datetime(2004,8,13,19),\n",
    "                            \"vmax\": 130. * units.kts,\n",
    "                            \"mslp\": 941. * units.mb\n",
    "                            }\n",
    "            self.landfall2 = {\"time\": dt.datetime(2004,8,13,20),\n",
    "                            \"vmax\": 125. * units.kts,\n",
    "                            \"mslp\": 942. * units.mb\n",
    "                            }\n",
    "        if name == 'HARVEY':\n",
    "            self.landfall = {\"time\": dt.datetime(2017,8,26,6),\n",
    "                            \"vmax\": 100. * units.kts,\n",
    "                            \"mslp\": 950. * units.mb\n",
    "                            }\n",
    "        if name == 'IRMA':\n",
    "            self.landfall = {\"time\": dt.datetime(2017,9,10,12),\n",
    "                            \"vmax\": 115. * units.kts,\n",
    "                            \"mslp\": 931. * units.mb\n",
    "                            }\n",
    "\n",
    "\n",
    "def get_bounds(domain):\n",
    "    # domain is something like 'stride02.-99.0E-87.0E25.0N31.0N'\n",
    "    words =  domain.split('.')\n",
    "    stride = words[0]\n",
    "    lonmin = float(words[1]+'.'+words[2][0:1])\n",
    "    lonmax = float(words[2][2:]+'.'+words[3][0:1])\n",
    "    latmin = float(words[3][2:]+'.'+words[4][0:1])\n",
    "    latmax = float(words[4][2:]+'.'+words[5][0:1])\n",
    "    return lonmin, lonmax, latmin, latmax\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Perturbation:\n",
    "    # Called by Perturbations\n",
    "    def __init__(self, path, storm, Ithresh, minus_astronomical_tide, dryland,\n",
    "                debug=False):\n",
    "        \n",
    "        bpath = os.path.basename(path)\n",
    "        self.bpath = bpath\n",
    "\n",
    "        if debug:\n",
    "            print(\"initializing Perturbation object\")\n",
    "            print(\"basename path= \"+bpath)\n",
    "        if 'control' in bpath:\n",
    "            self.value = bpath\n",
    "            self.valuelabel = 'control ('+bpath+')'\n",
    "        elif 'ens_' in bpath:\n",
    "            self.valuelabel = re.findall(r'ens_(\\d+)', bpath)[0]\n",
    "            self.value = int(self.valuelabel)\n",
    "        elif '.PF' in bpath: # 'ECMWF.0p125.2017090812.PF1'\n",
    "            self.valuelabel = re.findall(r'.PF(\\d+)', bpath)[0]\n",
    "            self.value = int(self.valuelabel)\n",
    "        elif '.CF0' in bpath: # 'ECMWF.0p125.2017090812.CF0'\n",
    "            self.valuelabel = 'CF0'\n",
    "            self.value = 0\n",
    "        elif 'WRF' in bpath:\n",
    "            # value label is number part of .EPS_45.\n",
    "            # The ensemble member\n",
    "            self.valuelabel = bpath[19:21]\n",
    "            self.value = bpath[4:]\n",
    "        else:\n",
    "            # Find last index of + or - sign\n",
    "            n = max([bpath.find('+'), bpath.find('-')])\n",
    "            if n == -1:\n",
    "                self.value = re.findall(r'\\d+', bpath)[0]\n",
    "                # good for 2017090600.uni\n",
    "                if self.value[0:3] == '201':\n",
    "                    self.value = dt.datetime.strptime(self.value,'%Y%m%d%H')\n",
    "            else:\n",
    "                bpath = bpath[n:]\n",
    "                self.value = float(bpath)\n",
    "                \n",
    "        if debug:\n",
    "            print(\"value=\",self.value)\n",
    "            print(\"valuelabel=\"+self.valuelabel)\n",
    "        self.path = path\n",
    "        \n",
    "        timeseries_search_str = path + \\\n",
    "            '/*.minus_astronomical_tide'+str(minus_astronomical_tide)+\\\n",
    "            '_'+Ithresh+'.'+dryland+'.'+storm.domain+'.timeseries.nc'\n",
    "\n",
    "        tsfile = glob.glob(timeseries_search_str)\n",
    "        if len(tsfile) != 1:\n",
    "            print(\"Did not find one file matching \"+timeseries_search_str)\n",
    "        \n",
    "        # Fill in these attributes of Perturbation object at time of max Inundation Volume:\n",
    "        # volume in control zone, length scale, inundation area, average depth\n",
    "        print(tsfile[0])\n",
    "        fh = Dataset(tsfile[0], mode='r')\n",
    "        inund = fh.variables['inundation_volume'][:]\n",
    "        imax = np.argmax(inund)\n",
    "        \n",
    "        self.max_vol         = inund[imax]\n",
    "\n",
    "        # True = old way (get value at time of max inundation volume over whole domain)\n",
    "        self.fixed_time      = False\n",
    "        if self.fixed_time:\n",
    "            self.max_vol_in_ctrl = fh.variables['volume_in_ctrl'][imax]\n",
    "            self.length_scale    = fh.variables['length_scale'][imax]\n",
    "            self.area            = fh.variables['inundation_area'][imax]\n",
    "            self.depth           = fh.variables['average_depth'][imax]\n",
    "        else: \n",
    "            # new way (look for max value over all times, not just time of max inundation volume over\n",
    "            # whole domain)\n",
    "            # For example, the time with the most water in the ctrl zone is not necessarily the same\n",
    "            # as the time when the total inundation volume is greatest.\n",
    "            # Use [:]. or else you sometimes get _FillValue.\n",
    "            self.max_vol_in_ctrl = np.max(fh.variables['volume_in_ctrl'][:])\n",
    "            self.length_scale    = np.max(fh.variables['length_scale'][:])\n",
    "            self.area            = np.max(fh.variables['inundation_area'][:])\n",
    "            self.depth           = np.max(fh.variables['average_depth'][:])\n",
    "\n",
    "\n",
    "        self.inund           = inund\n",
    "        \n",
    "        # assign time to time attribute\n",
    "        # use netCDF4-python num2date to convert numbers to datetime objects\n",
    "        nctimevar = fh.variables['time']\n",
    "        self.time = num2date(nctimevar[:], nctimevar.units)\n",
    "        fh.close()\n",
    "\n",
    "        # if vmax+3, keep +3 part\n",
    "        if isinstance(self.value,float):\n",
    "            self.valuelabel = '{0:+g}'.format(self.value)\n",
    "        # If datetime, keep month/day\n",
    "        elif isinstance(self.value,dt.datetime):\n",
    "            self.valuelabel = self.value.strftime('%m/%d')\n",
    "        #if self.value == 0:\n",
    "            #self.valuelabel = '' # Uncomment to not include control in legends\n",
    "\n",
    "        # Also get max water level field. Find node with highest inundation below 6.5 m\n",
    "        mxfile = path+'/maxele.63.nc'\n",
    "        Athresh = 6.5\n",
    "\n",
    "        try:\n",
    "            fh = Dataset(mxfile, mode='r')\n",
    "        except:\n",
    "            if not os.path.exists(mxfile):\n",
    "                print(\"maxele.63.nc not in\", path)\n",
    "            if os.path.islink(mxfile) and not os.path.exists(os.readlink(mxfile)):\n",
    "                print(\"maxele.63.nc symbolic link broken in\", path)\n",
    "            print(\"could not read\", self.mxfile) \n",
    "            self.mxfile = None\n",
    "            maxele = None\n",
    "            A = None\n",
    "            value = None\n",
    "            lon = None\n",
    "            lat = None\n",
    "        else:\n",
    "            self.mxfile = mxfile\n",
    "            maxele = fh.variables['zeta_max'][:]\n",
    "            # Find a node (largest index) with max water height < 6.5m, south of 40°N\n",
    "            A = np.argmax(maxele * (maxele<Athresh) * (fh.variables['y'][:]<40))\n",
    "            lon = fh.variables['x'][A]\n",
    "            lat = fh.variables['y'][A]\n",
    "            value = maxele[A]\n",
    "\n",
    "        fh.close()\n",
    "\n",
    "        self.maxele = maxele\n",
    "        self.pointA = {\"index\":A, \"value\":value, \"thresh\":Athresh, \"lon\":lon, \"lat\":lat}\n",
    "        print(path)\n",
    "\n",
    "        \n",
    "class Perturbations:\n",
    "    \n",
    "    # Returns an ordered list of perturbation instances in increasing order. \n",
    "    \n",
    "    def __init__(self, storm, ptype, punits=\"\", xlabel=\"\", Ithresh='1.00m', \n",
    "                 minus_astronomical_tide=False, dryland='MHHW', debug=False):\n",
    "\n",
    "        # Map perturbation type to subdirectory name.\n",
    "        if ptype == 'veers' :\n",
    "            pstr = 'veer'\n",
    "            if storm.name == 'HARVEY':\n",
    "                pstr = 'track'\n",
    "        elif ptype == 'speeds':\n",
    "            pstr = 'speed'\n",
    "        elif ptype == 'vmaxes':\n",
    "            pstr = 'vmax_PcAdjust'\n",
    "            if storm.name in ['IRMA','HARVEY'] :\n",
    "                pstr = 'vmax'\n",
    "        elif ptype == 'rmaxes':\n",
    "            pstr = 'rmax'\n",
    "        else:\n",
    "            print(\"Unknown perturbation type\", name)\n",
    "            sys.exit(1)\n",
    "        \n",
    "        if storm.name in ['IRMA','HARVEY']:\n",
    "            pstr = 'nws19.' + pstr\n",
    "            \n",
    "        # find all the directories, scrape off numeric part, sort numerically\n",
    "        dirnames = glob.glob(storm.basedir+pstr+'[+-]*[0-9]')\n",
    "        if debug:\n",
    "            print(\"Initializing Perturbations instance\")\n",
    "            print(\"type: \"+ptype)\n",
    "            print(\"perturbation string: \"+pstr)\n",
    "            print(\"dirnames:\",dirnames)\n",
    "\n",
    "        self.storm   = storm\n",
    "        self.ptype   = ptype\n",
    "        self.units   = punits\n",
    "        self.Ithresh = Ithresh\n",
    "        self.minus_astronomical_tide = minus_astronomical_tide\n",
    "\n",
    "        members = []\n",
    "        for i in dirnames:\n",
    "            members.append(Perturbation(i, storm, Ithresh, minus_astronomical_tide, dryland))\n",
    "            \n",
    "        self.members  = sorted(members, key=lambda x: x.value, reverse=False)\n",
    "        self.values          = [i.value for i in self.members]\n",
    "        self.dirnames        = [i.path for i in self.members]\n",
    "        self.area            = [i.area for i in self.members]\n",
    "        self.depth           = [i.depth for i in self.members]\n",
    "        self.domain          = storm.domain\n",
    "        self.dryland         = dryland\n",
    "        self.inund           = [i.inund for i in self.members]\n",
    "        self.length_scale    = [i.length_scale for i in self.members]\n",
    "        self.maxele          = [i.maxele for i in self.members]\n",
    "        self.max_vol_in_ctrl = [i.max_vol_in_ctrl for i in self.members]\n",
    "        self.max_vol         = [i.max_vol for i in self.members]\n",
    "        self.mxfile          = [i.mxfile for i in self.members]\n",
    "        self.pointA          = [i.pointA for i in self.members]\n",
    "        self.time            = [i.time for i in self.members]\n",
    "        self.valuelabels     = [i.valuelabel for i in self.members]\n",
    "        if self.units != \"\":\n",
    "            xlabel = \" (\"+self.units+\")\"\n",
    "        self.xlabel          = xlabel\n",
    "        self.index = 0\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        self.values = np.array(self.values)\n",
    "\n",
    "        # Get desc from attributes of length_scale variable in control run.\n",
    "        ifile = storm.basedir + storm.control_str + '/' + storm.control_str + \\\n",
    "            '.minus_astronomical_tide' + str(minus_astronomical_tide) + \\\n",
    "            '_' + Ithresh + '.' + dryland + '.' + storm.domain + '.timeseries.nc'\n",
    "        if debug:\n",
    "            print(\"ifile:\"+ifile)\n",
    "            print(\"members: \", self.members)\n",
    "\n",
    "        fh = Dataset(ifile, mode='r')\n",
    "        ls = fh.variables['length_scale']\n",
    "        self.desc = pstr + ' minus_astronomical_tide:' +str(minus_astronomical_tide) + \\\n",
    "            \" \" + dryland + '\\ninundation depth threshold: ' + \\\n",
    "            ls.depth_threshold +'\\nlength scale left side: ' + \\\n",
    "            str(ls.left_percentile) + '%; right: ' + str(ls.right_percentile) + '%'\n",
    "        fh.close()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.values)\n",
    "            \n",
    "    # Allow perturbations to be iterated over\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self.index < len(self.values):\n",
    "            member = self.members[self.index]\n",
    "            self.index += 1\n",
    "            return member\n",
    "        else:\n",
    "            self.index = 0\n",
    "            raise StopIteration\n",
    "    \n",
    "    def get_xticks(self):\n",
    "\n",
    "        # x-axis may be converted from perturbation magnitude to something else.\n",
    "\n",
    "        # Convert veer perturbations to cross-track distance right of landfall (km)\n",
    "        # Convert speed perturbations to along-track distance at landfall (km)\n",
    "        # Keep vmax perturbation (kts)\n",
    "        # Keep rmax perturbation (%)\n",
    "\n",
    "        stormname = self.storm.name\n",
    "\n",
    "        if self.ptype == 'veers':\n",
    "            if 'IKE' in stormname:\n",
    "                # From https://docs.google.com/a/ucar.edu/spreadsheets/d/1_Y9dX2240jMZ1abgZt4E6Td-pODq6k3q8rxVKwz55Us/edit?usp=sharing\n",
    "                pert,dist = np.array([(-6,-250),(-5,-198),(-4,-157),(-3,-121),(-2,-97),(-1,-49),(-0.5,-24),\n",
    "                                      (0.,0.),(0.5,22),(1.0,44),(2.0,82),(3.0,135),(4.0,146),(5.0,170),\n",
    "                                      (6.0,199)]).T\n",
    "            if stormname == 'CHARLEY':\n",
    "                # https://docs.google.com/a/ucar.edu/spreadsheets/d/1gKj72PWl8g_0QAOCMZawT2c8QRfDa1DreanUWU4DC74/edit?usp=sharing\n",
    "                # Spreadsheet lists these from +4.5 to -4.5\n",
    "                pert = [-4.5, -3.5, -2.5, -1.5, -0.5, 0, 0.5, 1.5, 2.5, 3.5, 4.5]\n",
    "                dist = [-343, -289, -153,  -63,  -15, 0,  32,  63, 102, 122, 134]\n",
    "            if stormname == 'IRMA':\n",
    "                # https://docs.google.com/a/ucar.edu/spreadsheets/d/1QtxUjO3qIXf2ySPRh0RPG51KasDUCQwmp4ac0hKG3zk/edit?usp=sharing\n",
    "                # Copied and pasted into vim window and edited to look like this.\n",
    "                pert,dist = np.array([(-7,-230), (-6,-195), (-5,-181), (-4,-136), (-3,-98), (-2,-63), (-1,-35),\n",
    "                                  (0,0), (1, 30), (2,60), (3,90), (4,128), (5,158), (6,188), (7,217)]).T\n",
    "            if stormname == 'HARVEY':\n",
    "                # https://docs.google.com/spreadsheets/d/1TBdFfGUwdJ9T5Oh7Kv2T7thGvZXeqU1Q_Laj0ztgN4U/edit?usp=sharing\n",
    "                pert,dist = np.array([(-7,-277.80), (-6,-255.58), (-5,-244.46), (-4,-118.53), (-3 ,-88.90),\n",
    "                                  (-2,-59.26), (-1,-29.63), (0,0.00), (1,31.48), (2,59.26), (3,96.30),\n",
    "                                  (4 ,118.53), (5 ,150.01), (6 ,177.79), (7 ,207.42)]).T\n",
    "\n",
    "\n",
    "        if self.ptype == 'speeds':\n",
    "            if 'IKE' in stormname:\n",
    "                # From https://docs.google.com/a/ucar.edu/spreadsheets/d/1_Y9dX2240jMZ1abgZt4E6Td-pODq6k3q8rxVKwz55Us/edit?usp=sharing\n",
    "                pert,dist = np.array([(-20,-401),(-15,-273),(-10,-173),(-5,-61),(0.,0.),(5,43),\n",
    "                                      (10,130),(15,248)]).T\n",
    "            if stormname == 'CHARLEY':\n",
    "                # From https://docs.google.com/a/ucar.edu/spreadsheets/d/1gKj72PWl8g_0QAOCMZawT2c8QRfDa1DreanUWU4DC74/edit?usp=sharing\n",
    "                pert = [ -15, -12.5,  -10, -7.5,   -5, -2.5, 0, 2.5,   5, 7.5,  10,  15]\n",
    "                dist = [-378,  -329, -260, -177, -116,  -35, 0,  94, 163, 220, 275, 348] # km\n",
    "            if stormname == 'IRMA':\n",
    "                # From https://docs.google.com/a/ucar.edu/spreadsheets/d/1QtxUjO3qIXf2ySPRh0RPG51KasDUCQwmp4ac0hKG3zk/edit?usp=sharing\n",
    "                # Copied and pasted into vim window and edited to look like this.\n",
    "                pert,dist = np.array([(-20,-314),(-15,-239), (-10,-159), (-5,-67), (0,0), (5,75), (10,154),\n",
    "                                      (15,200), (20,236)]).T\n",
    "            if stormname == 'HARVEY':\n",
    "                pert,dist = np.array([(-20,-125.936),(-15,-88.896),(-10,-64.82),(-5,-25.928),(0,0),(5,61.116),\n",
    "                                      (10,120.38),(15,164.828),(20,237.056)]).T\n",
    "\n",
    "\n",
    "        if self.ptype == 'vmaxes':\n",
    "            pert,dist = np.array([(-7,-28),(7,28)]).T\n",
    "\n",
    "        if self.ptype == 'rmaxes':\n",
    "            # Radius change of +100% error.\n",
    "            # Find landfall time in control/fort.22\n",
    "            # Find 34kt line. \n",
    "            # Look for max value in columns 35,36,37,38 (in nautical miles)\n",
    "            # Convert to km.\n",
    "            # IKE and CHARLEY from Kate Fossell's Feb 28 2017 email\n",
    "            if stormname ==    'IKE': rmax100 = 75.19 * units.km # km\n",
    "            if 'CHAR' in stormname:   rmax100 = 36.85 * units.km # km\n",
    "            if stormname ==   'IRMA': rmax100 = 60.93 * units.km # km\n",
    "            if stormname == 'HARVEY': rmax100 = 44.818 * units.km # km\n",
    "            pert,dist = list(zip(*[(-100.,-rmax100),(100.,rmax100)]))\n",
    "\n",
    "\n",
    "        if np.min(pert) >= 0:\n",
    "            print(\"no negative perturbations\", pert)\n",
    "            print(\"did you forget to negate the negative veers and speeds?\")\n",
    "            exit(2)\n",
    "\n",
    "        xticks = np.interp(self.values, pert, dist)\n",
    "        return xticks\n",
    "\n",
    "\n",
    "    # Standard prefix for output filename.\n",
    "    def oprefix(self, ptype=False):\n",
    "        p_namestr = ''\n",
    "        if ptype == True:\n",
    "            p_namestr = '.' + perturbations.ptype\n",
    "        prefix =  perturbations.storm.basedir + perturbations.storm.name + \\\n",
    "        p_namestr + \\\n",
    "        '.minus_astronomical_tide' + str(perturbations.minus_astronomical_tide) + \\\n",
    "        '.' + perturbations.Ithresh + '.' + perturbations.domain\n",
    "        return prefix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create storm objects with domains (must have been created by NCL scripts already)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IKE     = storm(name='IKE',     domain='stride10.-99.0E-87.0E25.0N31.0N')\n",
    "CHARLEY = storm(name=\"CHARLEY\", domain='stride02.-86.0E-78.0E24.2N30.0N')\n",
    "CHARIKE = storm(name='CHARIKE', domain='stride10.-99.0E-87.0E25.0N31.0N')\n",
    "\n",
    "init_date = '2017090812'\n",
    "control_str='nws20.control_newtides_' + init_date\n",
    "# limited number of inititalizations for control runs\n",
    "control_str='nws20.control_newtides_2017090812'\n",
    "\n",
    "\n",
    "HARVEY  = storm(name='HARVEY',  domain='stride02.-98.0E-92.2E24.8N30.5N', control_str='nws19.control')\n",
    "\n",
    "\n",
    "IRMA_SW      = storm(name='IRMA', domain='stride01.-88.0E-77.0E24.0N35.0NFloridaSW', control_str=control_str)\n",
    "IRMA_SWzoom2 = storm(name='IRMA', domain='stride02.-83.0E-79.0E24.0N27.0NFloridaSW', control_str=control_str)\n",
    "IRMA_SWzoom4 = storm(name='IRMA', domain='stride02.-82.3E-80.0E24.5N26.1NFloridaSW', control_str=control_str)\n",
    "IRMA_NEzoom3 = storm(name='IRMA', domain='stride02.-82.0E-78.2E30.0N33.0NFloridaNE', control_str=control_str)\n",
    "IRMA_NE      = storm(name='IRMA', domain='stride01.-88.0E-77.0E24.0N35.0NFloridaNE', control_str=control_str)\n",
    "\n",
    "\n",
    "thisstorm = IRMA_SW\n",
    "\n",
    "lonmin, lonmax, latmin, latmax = get_bounds(thisstorm.domain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stride01.-88.0E-77.0E24.0N35.0NFloridaSW\n"
     ]
    }
   ],
   "source": [
    "print(thisstorm.domain)\n",
    "from collections import OrderedDict\n",
    "import subprocess\n",
    "pCexe = \"/glade/u/home/fossell/adcirc/kowaleski/printClusters.csh\"\n",
    "\n",
    "\n",
    "def getClusters(init_date, cluster_start_time, cluster_deg, ncluster, debug=False):\n",
    "    # maybe use getClusters instead of printClusters\n",
    "    cluster_id = (init_date,cluster_start_time,cluster_deg,ncluster)\n",
    "    cyan  = [.13,1,.99]\n",
    "    green = [.12,1,.16]\n",
    "    magenta = [1,.11,.99]\n",
    "    orange = [1,.51,.15]\n",
    "    red = [.99,.1,.13]\n",
    "    blue = [.03,0,.98]\n",
    "    \n",
    "    cluster_color_dict = {}\n",
    "    #color that matches Alex's track plots\n",
    "    if cluster_id == ('2017090512', '2017090512', 5, 7):\n",
    "        cluster_color_dict = OrderedDict([\n",
    "            (1, cyan),\n",
    "            (2, \"Black\"),\n",
    "            (3, green),\n",
    "            (4, magenta),\n",
    "            (5, orange),\n",
    "            (6, red),\n",
    "            (7, blue)\n",
    "        ])\n",
    "    if cluster_id == ('2017090512', '2017090912', 3, 6):\n",
    "        cluster_color_dict = OrderedDict([\n",
    "            (5, red),     # (a)\n",
    "            (4, blue),    # (b)\n",
    "            (2, magenta), # (c)\n",
    "            (1, cyan),    # (d)\n",
    "            (6, orange),  # (e)\n",
    "            (3, green),   # (f)\n",
    "        ])\n",
    "    if cluster_id == ('2017090812', '2017090912', 3, 5):\n",
    "        cluster_color_dict = OrderedDict([\n",
    "            (3, red),     # (a)\n",
    "            (5, blue),    # (b)\n",
    "            (4, magenta), # (c)\n",
    "            (1, cyan),    # (d)\n",
    "            (2, orange),  # (e)\n",
    "        ])\n",
    "    \n",
    "    \n",
    "    if not cluster_color_dict:\n",
    "        print(\"bad cluster id\", cluster_id)\n",
    "        set_trace()\n",
    "\n",
    "    cluster_dict = OrderedDict()\n",
    "    iletter = 65 # chr(65)=\"A\" letter used in Kowaleski et al. paper\n",
    "    for cluster, color in cluster_color_dict.items():    \n",
    "        args = [pCexe, init_date, cluster_start_time, str(cluster_deg), str(ncluster), str(cluster)]\n",
    "        out = subprocess.run(args, check=True, stdout=subprocess.PIPE).stdout.decode()\n",
    "        # out can have multiple newlines (use replace, then strip)\n",
    "        # strip takes off trailing newline\n",
    "        if debug:\n",
    "            print(\"input to \"+pCexe, args)\n",
    "            print(\"output \"+out)\n",
    "        members = out.replace(\"\\n\",\" \").strip().split(' ')\n",
    "\n",
    "        # members 33 and 39 blow up\n",
    "        if init_date == '2017090512':\n",
    "            for badmember in ['33','39']:\n",
    "                if badmember in members:\n",
    "                    members.remove(badmember)\n",
    "\n",
    "        cluster_dict[cluster] = {\"members\":members,\"color\":color,\"letter\":chr(iletter)}\n",
    "        iletter = iletter+1\n",
    "\n",
    "    return cluster_dict\n",
    "\n",
    "def get_member_color(cluster_dict, member, debug=False):\n",
    "    # INPUT:\n",
    "    # cluster dictionary returned from getClusters()\n",
    "    # member\n",
    "    # RETURNS:\n",
    "    # member's color\n",
    "    \n",
    "    if not cluster_dict:\n",
    "        if debug:\n",
    "            print(\"cluster_color_dict is False. Returning Black\")\n",
    "        return \"Black\"\n",
    "    for cluster, cdict in cluster_dict:\n",
    "        members = cdict['members']\n",
    "        #print cluster, members\n",
    "        if member.lstrip('0') in members:\n",
    "            color = cluster_color_dict[cluster]\n",
    "            if debug:\n",
    "                print('color for member'+ member, color)\n",
    "            return color\n",
    "        else:\n",
    "            pass\n",
    "            #print member, 'not in', members\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## time-lagged MPAS ensemble, or ECMWF, or WRF ensemble  for IRMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search pattern: /glade/work/ahijevyc/ADCIRC/IRMA/WRF.2017090812.EPS_[0-9C][0-9O]*.27km3km\n",
      "initializing Perturbation object\n",
      "basename path= nws20.control_newtides_2017090812\n",
      "value= nws20.control_newtides_2017090812\n",
      "valuelabel=control (nws20.control_newtides_2017090812)\n",
      "Did not find one file matching /glade/work/ahijevyc/ADCIRC/IRMA/nws20.control_newtides_2017090812/*.minus_astronomical_tideFalse_1.00m.MHHW.stride01.-88.0E-77.0E24.0N35.0NFloridaSW.timeseries.nc\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m dryland  \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMHHW\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Start list of members with control \u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m members \u001b[38;5;241m=\u001b[39m [\u001b[43mPerturbation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthisstorm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasedir\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mthisstorm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrol_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthisstorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIthresh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mminus_astronomical_tide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdryland\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m]\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# or don't start with control\u001b[39;00m\n\u001b[1;32m     98\u001b[0m members \u001b[38;5;241m=\u001b[39m []\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mPerturbation.__init__\u001b[0;34m(self, path, storm, Ithresh, minus_astronomical_tide, dryland, debug)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDid not find one file matching \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mtimeseries_search_str)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Fill in these attributes of Perturbation object at time of max Inundation Volume:\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# volume in control zone, length scale, inundation area, average depth\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtsfile\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    108\u001b[0m fh \u001b[38;5;241m=\u001b[39m Dataset(tsfile[\u001b[38;5;241m0\u001b[39m], mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    109\u001b[0m inund \u001b[38;5;241m=\u001b[39m fh\u001b[38;5;241m.\u001b[39mvariables[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minundation_volume\u001b[39m\u001b[38;5;124m'\u001b[39m][:]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "ECMWF_ensemble = False # full grid MET forcing\n",
    "ECMWF_nws19_ensemble = False # parameterized storm\n",
    "MPAS_ensemble  = False\n",
    "WRF_ensemble   = True\n",
    "ECMWF_grid_spacing = False\n",
    "\n",
    "\n",
    "percentile = False\n",
    "vmax_from_besttrack = False\n",
    "besttrack_vmax_at_landfall = False\n",
    "besttrack_minp_vmax_at_landfall = False\n",
    "\n",
    "\n",
    "\n",
    "# functions naturally sort filenames with mixture of strings and integers\n",
    " \n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    '''\n",
    "    alist.sort(key=natural_keys) sorts in human order\n",
    "    http://nedbatchelder.com/blog/200712/human_sorting.html\n",
    "    (See Toothy's implementation in the comments)\n",
    "    '''\n",
    "    return [ atoi(c) for c in re.split(r'(\\d+)', text) ]\n",
    "\n",
    "\n",
    "if thisstorm.name == 'IRMA':\n",
    "\n",
    "    cluster_start_time = '2017090912' # probably '2017090912'\n",
    "    cluster_deg = 3\n",
    "    if init_date == '2017090512': ncluster = 6\n",
    "    if init_date == '2017090812': ncluster = 5\n",
    "\n",
    "    \n",
    "    if ECMWF_grid_spacing:\n",
    "        dirnames = sorted(glob.glob(thisstorm.basedir+'ECMWF.0p*'+init_date+'.ens_48'),\n",
    "                         key=natural_keys)\n",
    "        members_title = \" \".join([\"ECMWF ensemble grid spacing\", init_date, \"member 48\"])\n",
    "    if ECMWF_nws19_ensemble:\n",
    "        estr = 'ECMWF.0p125.nws19.'\n",
    "        tstr = ''\n",
    "        if percentile:\n",
    "            estr += '90percentile.'\n",
    "            tstr = '\\n90th percentile ' # space at end\n",
    "        if vmax_from_besttrack:\n",
    "            estr += 'vmax_from_besttrack.'\n",
    "            tstr = '\\nvmax from best track ' # space at end\n",
    "        if besttrack_vmax_at_landfall:\n",
    "            estr += 'besttrack_vmax_at_25.2N.'\n",
    "            tstr = '\\nbesttrack vmax at 25.2N '\n",
    "        if besttrack_minp_vmax_at_landfall:\n",
    "            estr += 'besttrack_minp_vmax_at_25.2N.'\n",
    "            tstr = '\\nbesttrack minp vmax at 25.2N '\n",
    "        basedir_search_str = thisstorm.basedir+estr+init_date+'.[PC]F[0-9]*'\n",
    "        dirnames = sorted(glob.glob(basedir_search_str), key=natural_keys)\n",
    "        #dirnames = dirnames[22:] # HACK - only ran members 1-22 to keep MMM under 100% 30-day utilization\n",
    "        if len(dirnames) == 0:\n",
    "            print(\"ECMWF nws19 ensemble basedir: \"+ basedir_search_str)\n",
    "            print(\"ECMWF nws19 ensemble dirnames=\", dirnames)\n",
    "        members_title = \"ECMWF ensemble 0p125 nws19 \"+tstr + init_date\n",
    "    if ECMWF_ensemble:\n",
    "        basedir_search_str = thisstorm.basedir+'ECMWF.0p125.'+init_date+'.[PC]F[0-9]*'\n",
    "        dirnames = sorted(glob.glob(basedir_search_str), key=natural_keys)\n",
    "        if len(dirnames) == 0:\n",
    "            print(\"ECMWF ensemble basedir: \"+ basedir_search_str)\n",
    "            print(\"ECMWF ensemble dirnames=\", dirnames)\n",
    "        members_title = \"ECMWF ensemble 0p125 \" + init_date\n",
    "    if MPAS_ensemble:\n",
    "        members_title = \"MPAS uniform 15km mesh, ECMWF init. conditions\"\n",
    "    if WRF_ensemble:\n",
    "        # Previously, I changed '??' to [0-9][0-9] to avoid matching 'CO'. Wanted to match 51, which was symbolic linked to 'CO'\n",
    "        # as of Dec 16 2019, removed reference to member 51 in directory structure and printCluster.csh output.\n",
    "        # So search for [0-9C][0-9O] instead of [0-9][0-9]\n",
    "        search_pattern = thisstorm.basedir+'WRF.' + init_date + '.EPS_[0-9C][0-9O]*.27km3km'\n",
    "        print('search pattern:', search_pattern)\n",
    "        dirnames = sorted(glob.glob(search_pattern))\n",
    "\n",
    "        if init_date == '2017090512':\n",
    "            # runs that blow up\n",
    "            dirnames.remove('/glade/work/ahijevyc/ADCIRC/IRMA/WRF.2017090512.EPS_33.27km3km')\n",
    "            dirnames.remove('/glade/work/ahijevyc/ADCIRC/IRMA/WRF.2017090512.EPS_39.27km3km')\n",
    "\n",
    "        members_title = 'WRF ' + init_date + ' EPS 27km3km'\n",
    "        \n",
    "    members_title_keep = members_title\n",
    "\n",
    "\n",
    "    Ithresh = '1.00m'\n",
    "    minus_astronomical_tide = False\n",
    "    dryland  = 'MHHW'\n",
    "\n",
    "    # Start list of members with control \n",
    "    members = [Perturbation(thisstorm.basedir+thisstorm.control_str, thisstorm, Ithresh, \n",
    "                            minus_astronomical_tide, dryland, debug=True)]\n",
    "    # or don't start with control\n",
    "    members = []\n",
    "    \n",
    "    for dirname in dirnames:        \n",
    "        members.append(Perturbation(dirname, thisstorm, Ithresh, minus_astronomical_tide, dryland, debug=True))\n",
    "\n",
    "\n",
    "    dirnames        = [i.path for i in members]\n",
    "    area            = [i.area for i in members]\n",
    "    depth           = [i.depth for i in members]\n",
    "    inund           = [i.inund for i in members]\n",
    "    length_scale    = [i.length_scale for i in members]\n",
    "    maxele          = [i.maxele for i in members]\n",
    "    max_vol_in_ctrl = [i.max_vol_in_ctrl for i in members]\n",
    "    max_vol         = [i.max_vol for i in members]\n",
    "    mxfile          = [i.mxfile for i in members]\n",
    "    pointA          = [i.pointA for i in members]\n",
    "    time            = [i.time for i in members]\n",
    "    valuelabels     = [i.value for i in members]\n",
    "\n",
    "def nice_grid(ax):\n",
    "    # major and minor ticks, gridlines. \n",
    "    ax.grid(b=True, which='major', color='#777777', linestyle='-', linewidth=0.5)\n",
    "    ax.xaxis.set_major_formatter(dates.DateFormatter('%-m/%-d'))\n",
    "    ax.xaxis.set_major_locator(dates.HourLocator(byhour=[0]))\n",
    "    ax.minorticks_on()\n",
    "    ax.grid(b=True, which='minor', color='#999999', linestyle='-', linewidth=0.5, alpha=0.2)\n",
    "    ax.xaxis.set_minor_locator(dates.HourLocator(byhour=[0,6,12,18]))\n",
    "    #fig.autofmt_xdate() # make tick labels slanty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series of inundation volume\n",
    "could be from special time-lagged IRMA ensemble by MPAS<br>\n",
    "could be ECMWF ensemble<BR>\n",
    "could be WRF ensemble from Alex K. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure variable is defined\n",
    "try:\n",
    "    members\n",
    "except NameError:\n",
    "    members = None\n",
    "\n",
    "# avoid this FutureWarning: Using an implicitly registered datetime converter for a matplotlib plotting method. \n",
    "# The converter was registered by pandas on import. \n",
    "# Future versions of pandas will require you to explicitly register matplotlib converters.\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "do_clusters = False\n",
    "\n",
    "if do_clusters and WRF_ensemble:\n",
    "    clusters = getClusters(init_date, cluster_start_time, cluster_deg, ncluster, debug=True)\n",
    "else:\n",
    "    # Merge clusters into a dictionary with single key 'all' \n",
    "    # (for non-cluster stuff like ECMWF ensemble)\n",
    "    clusters = {'all': {'members': [x.valuelabel for x in members], 'color':'black'}}\n",
    "\n",
    "clean = True # fewer symbols along lines\n",
    "sideHistogram = False # histogram of maxima on right\n",
    "npanel = ncluster\n",
    "ncols = 2\n",
    "allOnePanel = not do_clusters\n",
    "if allOnePanel:\n",
    "    npanel = 1\n",
    "    ncols  = 1\n",
    "nrows = int(np.ceil(npanel/ncols))\n",
    "border = 1.0\n",
    "figsize = (ncols*10./3.+border,nrows*7./2.+border)\n",
    "fig, ax = plt.subplots(ncols=ncols,nrows=nrows,figsize=figsize, \n",
    "                       squeeze=False, sharex=True, sharey=True) # squeeze=False returns 2D array not scalar\n",
    "plt.subplots_adjust(wspace = 0.1, hspace= 0.1)\n",
    "\n",
    "nullfmt = NullFormatter() # no labels for side histogram\n",
    "noticks = NullLocator()\n",
    "\n",
    "members_title = members_title_keep\n",
    "fineprint = \"\"\n",
    "for iax, (cluster, cdict) in enumerate(clusters.items()):\n",
    "    if not allOnePanel:\n",
    "        tsaxes = ax.flat[iax]\n",
    "    else:\n",
    "        tsaxes = ax.flat[0]\n",
    "    \n",
    "    tsaxes.tick_params(axis='both', which='major', labelsize='small')\n",
    "    if sideHistogram:\n",
    "        pos = tsaxes.get_position()\n",
    "        rect_histy = [pos.xmax + 0.025, pos.ymin, 0.05, pos.height]\n",
    "        axHisty = plt.axes(rect_histy)\n",
    "        axHisty.yaxis.set_major_formatter(nullfmt)\n",
    "        axHisty.set_title('maxima\\nhistogram', fontdict={'fontsize':'xx-small'})\n",
    "        axHisty.tick_params(axis='both', which='major', labelsize='xx-small')\n",
    "\n",
    "    maxima = []\n",
    "    # Only Alex K's WRF ensemble deals with clusters\n",
    "    color = 'black'\n",
    "    if WRF_ensemble:\n",
    "        cluster_str = str(cluster)\n",
    "        if allOnePanel:\n",
    "            cluster_str = \"all\"\n",
    "        cluster_id = ' '.join([\n",
    "            init_date,\n",
    "            cluster_start_time,\n",
    "            str(cluster_deg),\n",
    "            str(ncluster),\n",
    "            cluster_str\n",
    "        ])\n",
    "        members_title = members_title_keep.replace('EPS','EPS cluster'+cluster_id)\n",
    "        if do_clusters:\n",
    "            color = cdict['color']\n",
    "        else:\n",
    "            color=\"gray\"\n",
    "        cluster_members = cdict['members']\n",
    "        \n",
    "    elif MPAS_ensemble or ECMWF_ensemble or ECMWF_nws19_ensemble or ECMWF_grid_spacing:\n",
    "        cluster_id = \"none\"\n",
    "        # list of strings of numbers 1 - len(members)\n",
    "        cluster_members = clusters['all']['members']\n",
    "    else:\n",
    "        print(\"unknown ensemble\")\n",
    "        set_trace()\n",
    "        \n",
    "    if members:\n",
    "\n",
    "        for pert in members:\n",
    "            label = pert.bpath\n",
    "            label = pert.valuelabel\n",
    "            line, = tsaxes.plot(pert.time,pert.inund,label=label,markersize=3,lw=0.7)\n",
    "            line.set_color(color)\n",
    "            line.set_alpha(0.6)\n",
    "\n",
    "            # label/star top of line\n",
    "            top_of_line = np.max(pert.inund)\n",
    "            maxima.append(top_of_line)\n",
    "            print(pert.valuelabel, pert.time[np.argmax(pert.inund)], top_of_line)\n",
    "        \n",
    "            if clean:\n",
    "                label, = tsaxes.plot(pert.time[np.argmax(pert.inund)], top_of_line, 'k*')\n",
    "            else:\n",
    "                label = tsaxes.annotate(pert.valuelabel,\n",
    "                    xy=(pert.time[np.argmax(pert.inund)], top_of_line), xycoords='data',\n",
    "                    xytext=(0, 0), textcoords='offset points', fontsize=3.5, color=line.get_color(),\n",
    "                    horizontalalignment='center', verticalalignment='bottom')\n",
    "\n",
    "\n",
    "            if isinstance(pert.value, str) and 'control' in pert.value:\n",
    "                line.set_linewidth(4)\n",
    "                line.set_color('k')\n",
    "                line.set_alpha(1)\n",
    "                # Remove this maximum from maxima list\n",
    "                maxima.remove(top_of_line)\n",
    "                if sideHistogram:\n",
    "                    hline = axHisty.axhline(top_of_line)\n",
    "                    hline.set_linewidth(line.get_linewidth())\n",
    "                    hline.set_color(line.get_color())\n",
    "                fineprint += 'control='+thisstorm.control_str\n",
    "            elif isinstance(pert.value, dt.datetime):\n",
    "                if pert.value > dt.datetime(2017,9,7):\n",
    "                    line.set_marker('o')\n",
    "                    line.set_markevery(1)\n",
    "            elif isinstance(pert.value, int):\n",
    "                markers=False\n",
    "                if markers:\n",
    "                    markers = ['','o','^','s','x']\n",
    "                    line.set_marker(markers[int((pert.value-1)/10)])\n",
    "                    line.set_markevery(12)\n",
    "\n",
    "\n",
    "\n",
    "            # Make non-member lines invisible\n",
    "            # strip zero-pad before comparing?\n",
    "            #if pert.valuelabel.lstrip('0') not in cluster_members:\n",
    "            if pert.valuelabel not in cluster_members:\n",
    "                print(\"cluster members\", cluster_members)\n",
    "                print('hiding', pert.valuelabel.lstrip('0'))\n",
    "                line.set_visible(False)\n",
    "                label.set_visible(False)\n",
    "\n",
    "\n",
    "        ylabel = 'Inundation volume ($\\mathregular{km^3}$)'\n",
    "        tsaxes.set_ylabel(ylabel)\n",
    "\n",
    "        nice_grid(tsaxes)\n",
    "\n",
    "\n",
    "        tsaxes.set_xlabel(members[0].time[0].strftime(\"%Y\"))\n",
    "\n",
    "        if not clean: # used to have not allOnePanel too\n",
    "            # put legend\n",
    "            if len(members) > 10:\n",
    "                tsaxes.legend(loc='best', fontsize=4, ncol=4)\n",
    "            else:\n",
    "                tsaxes.legend(loc='best')\n",
    "\n",
    "            plt.suptitle(thisstorm.name)\n",
    "            tsaxes.set_title(members_title, fontsize='x-small')\n",
    "        else:\n",
    "            if do_clusters:\n",
    "                panel_letter = chr(iax+97)\n",
    "                if cdict['letter'].upper() != panel_letter.upper():\n",
    "                    print(\"cluster letter doesn't match expectations\")\n",
    "                    pdb.set_trace()\n",
    "                panel_str = '('+panel_letter+')'\n",
    "                tsaxes.annotate(panel_str, (0.01, 0.98), xycoords='axes fraction',\n",
    "                               fontsize=28, verticalalignment='top')\n",
    "\n",
    "\n",
    "        # force 2017090812 and 2017090512 to same limits\n",
    "        print(tsaxes.get_xlim())\n",
    "        print(tsaxes.get_ylim())\n",
    "        if thisstorm.domain == 'stride01.-88.0E-77.0E24.0N35.0NFloridaNE':\n",
    "            tsaxes.set_xlim((736581., 736585.))\n",
    "            tsaxes.set_ylim((-0.14114182363104172, 4.958379911452546))\n",
    "        if thisstorm.domain == 'stride01.-88.0E-77.0E24.0N35.0NFloridaSW':\n",
    "            tsaxes.set_xlim((736581., 736585.))\n",
    "            tsaxes.set_ylim((-0.14205421008031377, 4.6))\n",
    "            if WRF_ensemble: tsaxes.set_ylim((-0.14205421008031377, 3.))\n",
    "\n",
    "\n",
    "\n",
    "        # Add side histogram. Do this after main axes limits are settled.\n",
    "        if sideHistogram:\n",
    "            count, bins, patches = axHisty.hist(maxima, orientation='horizontal')\n",
    "            axHisty.set_ylim(tsaxes.get_ylim())\n",
    "\n",
    "for i in ax.flat:\n",
    "    i.label_outer()\n",
    "            \n",
    "# Hide axes we don't need\n",
    "if clean and ncluster % 2 != 0 and len(ax.flat) > 1:\n",
    "    ax[-1,-1].axis('off')\n",
    "    # restore xtick labels to right column.\n",
    "    ax[-2,-1].xaxis.set_tick_params(labelbottom=True)\n",
    "\n",
    "\n",
    "fineprint += \"\\ncluster \" + cluster_id +\"\\n\"+\" \".join([\"ensemble\", thisstorm.domain])\n",
    "if clean:fineprint = \"\"\n",
    "status = mysavfig(thisstorm.basedir+members_title.replace(' ','_').replace('\\n','.') + '.' \n",
    "                  + thisstorm.domain + '.timeseries.png', \n",
    "                  string = fineprint, \n",
    "                  **savfig_dict )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Perturbation(thisstorm.basedir+thisstorm.control_str, thisstorm, Ithresh, minus_astronomical_tide, dryland)\n",
    "[x.value for x in members]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allOnePanel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define map_points() for Basemap plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "\n",
    "def map_points(ax, lon, lat, s=64, scalebuffer=1., colors='k', linewidth=0.5, resolution='i', \n",
    "               labels=None, debug=False, **kwargs):\n",
    "    # Scatter plot on a map.\n",
    "    # Domain large enough to encompasses all the markers, plus a 3° longitude and 1° latitude buffer. \n",
    "    m = Basemap(lon_0=-90,lat_0=30,resolution=resolution,projection='stere',\\\n",
    "                llcrnrlat=np.amin(lat)-1.*scalebuffer,\\\n",
    "                urcrnrlat=np.amax(lat)+1.*scalebuffer,\\\n",
    "                llcrnrlon=np.amin(lon)-3.*scalebuffer,\\\n",
    "                urcrnrlon=np.amax(lon)+3.*scalebuffer,ax=ax)\n",
    "    m.drawcoastlines(linewidth=0.5)\n",
    "    # fill continents, set lake color same as ocean color.\n",
    "    m.fillcontinents(color='white',lake_color='aqua',zorder=0)\n",
    "    # draw parallels and meridians.\n",
    "    # label parallels on right and top\n",
    "    # meridians on bottom and left\n",
    "    parallels = np.arange(0.,81,1.)\n",
    "    # labels option turns off/on label. 4 element list for 4 sides: [left,right,top,bottom]\n",
    "    m.drawparallels(parallels,labels=[True,True,False,True], linewidth=0.5)\n",
    "    meridians = np.arange(10.,351.,2.)\n",
    "    m.drawmeridians(meridians,labels=[True,True,False,True], linewidth=0.5)\n",
    "    if debug:\n",
    "        print(\"drawing state boundaries\")\n",
    "    m.drawstates()\n",
    "    m.scatter(lon, lat, latlon=True, s=s, c=colors, linewidth=linewidth, **kwargs)\n",
    "    if labels:\n",
    "        for x, y, label in zip(lon, lat, labels):\n",
    "            ax.annotate(label, xy=m(x, y), ha='center', va='center', fontsize=5.)\n",
    "\n",
    "\n",
    "    return m\n",
    "\n",
    "def map_track(m, trackfile, color='black', debug=False):\n",
    "    if debug:\n",
    "        print(\"map_track: genfromtxt() with \"+trackfile)\n",
    "    lat0, lon0 = np.genfromtxt(trackfile,delimiter=',', usecols=(6,7),dtype=None, unpack=True, encoding=\"utf-8\")\n",
    "    lat0 = [int(i[:-1])/10. for i in lat0]\n",
    "    lon0 = [int(i[:-1])/ -10. for i in lon0]\n",
    "    track = m.plot(lon0, lat0, color=color, latlon=True, label=trackfile[70:74],\n",
    "                   linewidth=(3 if 'control' in trackfile else .71))\n",
    "    return track\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "def forecast_err_box(xmin, xmax, label, **kwargs):\n",
    "    # Called by forecast_err_boxes()\n",
    "    # Draw box from xmin to xmax, spanning the vertical.\n",
    "    # Label vertical lines too.\n",
    "    ax = kwargs['axes']\n",
    "    # Dont' change the xaxis range\n",
    "    ax.autoscale(False)\n",
    "    box = ax.axvspan(xmin, xmax, alpha=0.075, facecolor='red', **kwargs)\n",
    "    edgecolor = 'red'\n",
    "    box = ax.axvspan(xmin, xmax, alpha=0.2, facecolor=\"none\", edgecolor=edgecolor, lw=1.8, **kwargs)\n",
    "    labelL = ax.annotate(s=label, xy=(xmin,0.5), ha=\"center\", va=\"center\", xycoords=('data', 'axes fraction'), \n",
    "                         rotation=90, fontsize=12, annotation_clip=True, zorder=1, alpha=0.75, **kwargs)\n",
    "    labelR = ax.annotate(s=label, xy=(xmax,0.5), ha=\"center\", va=\"center\", xycoords=('data','axes fraction'), \n",
    "                         rotation=90, fontsize=12, annotation_clip=True, zorder=1, alpha=0.75, **kwargs)\n",
    "\n",
    "def forecast_err_boxes(ax, perturbation):\n",
    "    # Draw tranparent red boxes to illustrate typical forecast err for different lead times.\n",
    "\n",
    "    if perturbation.ptype == 'veers':\n",
    "        # list of (hour, nautical miles) tuples to define 2010-2014 cone from \n",
    "        # https://docs.google.com/a/ucar.edu/spreadsheets/d/1_Y9dX2240jMZ1abgZt4E6Td-pODq6k3q8rxVKwz55Us/edit?usp=sharing\n",
    "        fhr, radii = list(zip(*[(12,32),(24,52),(36,71),(48,90),(72,122),(96,170),(120,225)]))\n",
    "        \n",
    "        # Convert nautical miles to km\n",
    "        radii = units[\"nautical miles\"] * radii\n",
    "        radii = radii.to('km')\n",
    "        fhrboxes = [12,24,48,72]\n",
    "\n",
    "    if perturbation.ptype == 'speeds':\n",
    "        # list of (hour, km) tuples to define 2010-2014 cone estimated to be more than cross-track error from\n",
    "        # https://docs.google.com/a/ucar.edu/spreadsheets/d/1_Y9dX2240jMZ1abgZt4E6Td-pODq6k3q8rxVKwz55Us/edit?usp=sharing\n",
    "        fhr, radii = list(zip(*[(12,32),(24,52),(36,71),(48,90),(72,122),(96,170),(120,225)]))\n",
    "\n",
    "        # Convert nautical miles to km\n",
    "        radii = units[\"nautical miles\"] * radii\n",
    "        radii = radii.to('km')\n",
    "\n",
    "        fhrboxes = [12,24,48,72]\n",
    "\n",
    "    if perturbation.ptype == 'vmaxes':\n",
    "        # list of (hour, knots) tuples from NHC published stats \n",
    "        # https://drive.google.com/open?id=0B4GoIuq38OVyLTRhTkxmbE1YZVY1eU5xYlA3RlJWOHlyN3A0\n",
    "        fhr, radii = list(zip(*[(24,10),(48,15),(72,20)]))\n",
    "        fhrboxes = fhr\n",
    "        \n",
    "    if perturbation.ptype == 'rmaxes':\n",
    "        # list of (hour, km) tuples Feb 10 2017 email from Kate Fossell\n",
    "        # Based on Cangialosi and Landsea (2016)\n",
    "        fhr, radii = list(zip(*[(12,37),(24,52),(36,60),(48,63),(72,68)]))\n",
    "        fhrboxes = [12,24,48,72]\n",
    "        \n",
    "    try:\n",
    "        fhr\n",
    "    except NameError:\n",
    "        print(\"no foreast lead time window for\", perturbation.name, perturbation.storm.name)\n",
    "        return\n",
    "    # np.interp(x,xp,fp)\n",
    "    # x: x-coordinates of interpolated values\n",
    "    # xp: 1-D seq of floats. the x-coordinates of the data points\n",
    "    # fp: 1-D seq of floats or complex. The y-coordinates of the data points, same len as xp.\n",
    "    # Get errors assocated with fboxes list.\n",
    "    lefts = -np.interp(fhrboxes, fhr, radii)\n",
    "    rights = np.interp(fhrboxes, fhr, radii)\n",
    "    for box, left, right in zip(fhrboxes, lefts, rights):\n",
    "        junk = forecast_err_box(left, right, '%d-hr' % box, axes=ax)\n",
    "        \n",
    "def zeroLine(line, **kwargs):\n",
    "    line.axes.axvline(x=0, linestyle='dashed', color='black', zorder=1)\n",
    "    # Use np.isclose because of rmaxes, where zero tick is interpolated to a tiny non-zero value.\n",
    "    i0, = np.where(np.isclose(line.get_xdata(), 0))\n",
    "    line.axes.axhline(y=line.get_ydata()[i0], linestyle='dashed', color=line.get_color(), **kwargs)\n",
    "            \n",
    "def annotate_pts(perturbation, line):\n",
    "    for label, xypoints in zip(perturbation.valuelabels, line.get_xydata()):\n",
    "        line.axes.annotate(label, xy=xypoints, textcoords='offset points', xytext=(0,+4), \n",
    "                           ha='center', va='bottom', fontsize=6.5)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Best Track run to NOAA Stations (IKE or IRMA )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOAA station data come from tidesandcurrents.noaa.gov\n",
    "def turl(station_id, begin_date, end_date, datum):\n",
    "    url = \"https://tidesandcurrents.noaa.gov\"\n",
    "    \n",
    "    search_string = \"/api/datagetter?product=water_level&application=NOS.COOPS.TAC.WL&station=\"+\\\n",
    "        station_id+\"&begin_date=\"+begin_date+\"&end_date=\"+end_date+\"&datum=\"+datum+\\\n",
    "        \"&units=metric&time_zone=GMT&format=csv\"\n",
    "\n",
    "    return url + search_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from netCDF4 import MFDataset\n",
    "import xarray\n",
    "\n",
    "\n",
    "datums = {}\n",
    "# TODO: get directly from tidesandcurrents.noaa.gov\n",
    "# meters relative to MLLW\n",
    "datums['8665530'] = {'MHHW': 1.757, 'NAVD88': 0.957, 'name':'Charleston, Cooper River Entrance, SC'}\n",
    "datums['8670674'] = {'MHHW': 0.000, 'NAVD88': 0.000, 'name':'Talmadge Memorial Bridge, GA'}\n",
    "datums['8670870'] = {'MHHW': 2.287, 'NAVD88': 1.235, 'name':'Fort Pulaski, GA'}\n",
    "datums['8720030'] = {'MHHW': 1.999, 'NAVD88': 1.165, 'name':'Fernandina Beach, FL'}\n",
    "datums['8722956'] = {'MHHW': 0.857, 'NAVD88': 0.694, 'name':'South Port Everglades, FL'}\n",
    "datums['8723214'] = {'MHHW': 0.676, 'NAVD88': 0.614, 'name':'Virginia Key, Biscayne Bay, FL'}\n",
    "datums['8723970'] = {'MHHW': 0.297, 'NAVD88': 0.407, 'name':'Vaca Key, Florida Bay, FL'}\n",
    "datums['8724580'] = {'MHHW': 0.551, 'NAVD88': 0.538, 'name':'Key West, FL'}\n",
    "datums['8725110'] = {'MHHW': 0.874, 'NAVD88': 0.690, 'name':'Naples, Gulf Of Mexico FL'}\n",
    "datums['8726667'] = {'MHHW': 0.816, 'NAVD88': 0.508, 'name':'Mckay Bay Entrance, FL'}\n",
    "\n",
    "def to_MHHW(cluster_zeta, debug=False):\n",
    "    # cluster_zeta is an xarray DataArray\n",
    "    MHHW_long_name = 'mean higher-high water'\n",
    "    if cluster_zeta.attrs['long_name'] == MHHW_long_name:\n",
    "        if debug:\n",
    "            print(\"to_MHHW: cluster_zeta already in MHHW. returning unchanged\")\n",
    "        return cluster_zeta\n",
    "    \n",
    "    station_id = str(cluster_zeta.station.values)\n",
    "    if cluster_zeta.attrs['long_name'] != \"water surface elevation above geoid\":\n",
    "        print(\"unexpected long_name \"+cluster_zeta.long_name)\n",
    "        sys.exit(1)\n",
    "    if cluster_zeta.attrs['standard_name'] != \"sea_surface_height_above_geoid\":\n",
    "        print(\"unexpected standard_name \"+cluster_zeta.standard_name)\n",
    "        sys.exit(1)\n",
    "    if cluster_zeta.attrs['units'] != \"m\":\n",
    "        print(\"unexpected units \"+cluster_zeta.units)\n",
    "        sys.exit(1)\n",
    "        \n",
    "    if debug:\n",
    "        print(\"to_MHHW: converting \"+station_id+\" to MHHW\")\n",
    "\n",
    "    # Adding NAVD88 converts from NAVD88 to MLLW\n",
    "    # subtracting MHHW converts from MLLW to MHHW\n",
    "    try:\n",
    "        NAVD88 = datums[station_id]['NAVD88']\n",
    "        MHHW   = datums[station_id]['MHHW']\n",
    "    except KeyError:\n",
    "        print(\"no hard-coded datum for station \"+station_id+\". We have\")\n",
    "        print(datums)\n",
    "        print(\"copy and paste from tidesandcurrents.noaa.gov\")\n",
    "        sys.exit(1)\n",
    "    cluster_zeta.values = cluster_zeta.values + NAVD88 - MHHW\n",
    "    cluster_zeta.attrs['long_name'] = 'mean higher-high water'\n",
    "    cluster_zeta.attrs['standard_name'] = 'MHHW'\n",
    "    return cluster_zeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'{:.2f}'.format(cluster_zeta.max().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_clusters = True\n",
    "debug=False\n",
    "\n",
    "if True and thisstorm.name in ['IKE','IRMA']:\n",
    "    if do_clusters:\n",
    "        searchstr = thisstorm.basedir+'WRF.'+init_date+'.EPS_[0-9C][0-9O].27km3km/fort.61.nc'\n",
    "        if debug:\n",
    "            print(\"searching \"+searchstr)\n",
    "        fort61 = glob.glob(searchstr)\n",
    "    else:\n",
    "        fort61 = [thisstorm.basedir+thisstorm.control_str+'/fort.61.nc']\n",
    "\n",
    "    # Get stations, lat/lon, time, and zeta from first model file\n",
    "    first_model_file = fort61[0]\n",
    "    ds = xarray.open_dataset(first_model_file)\n",
    "\n",
    "    begin_date = pd.to_datetime((ds.time.min() + np.timedelta64(4,'D')).values)\n",
    "    begin_date = begin_date.strftime('%Y%m%d')\n",
    "    # Don't do offset from ds.time.min(). There are different initialization times\n",
    "    begin_date = '20170909'\n",
    "    #end_date = ds.time.max() + np.timedelta64(24, 'h') # must be multiple of days\n",
    "    #end_date = pd.to_datetime(end_date.values).strftime('%Y%m%d')\n",
    "    end_date = '20170913'\n",
    "\n",
    "    #units = cf_units.Unit(ds.zeta.units)\n",
    "    #long_name = ds.zeta.long_name\n",
    "    ds.close()\n",
    "    \n",
    "    # add zeta from other model files (if any)\n",
    "    if debug: print(\"concat'ing\",end=\" \")\n",
    "    for station_file in fort61[1:]: # we already read 1st model file\n",
    "        if debug:\n",
    "            print(os.path.basename(station_file), end=' ')\n",
    "        iEPS = station_file.index('EPS_')\n",
    "        member = station_file[iEPS+4:iEPS+6]\n",
    "        print(\"(member \"+member+\")\")\n",
    "        ds1 = xarray.open_dataset(station_file)\n",
    "        # make the array of station names the coordinate variable for station dimension\n",
    "        station = [ x.values.tostring().decode(encoding=\"utf-8\").strip() for x in ds1.station_name]\n",
    "        ds1 = ds1.assign_coords(station = station)\n",
    "        ds = xarray.concat([ds,ds1], dim=\"member\") \n",
    "        ds1.close()\n",
    "    print(ds.zeta.shape)\n",
    "    ds = ds.assign_coords(member = [station_file[iEPS+4:iEPS+6] for station_file in fort61])\n",
    "\n",
    "    # truncate to time axis extent, so max() function won't find time outside of time axis\n",
    "    ds = ds.sel(time=slice(dt.datetime.strptime(begin_date, '%Y%m%d'), dt.datetime.strptime(end_date, '%Y%m%d')))\n",
    "    \n",
    "    \n",
    "    opener = urllib.request.build_opener(cache.CacheHandler(\".urllib2cache\"))\n",
    "\n",
    "    ncol = 2\n",
    "    nrow = ncluster\n",
    "    figsize=(16,ncluster*4)\n",
    "    if clean:\n",
    "        nrow = int(np.ceil(ncluster/ncol))\n",
    "        figsize=(10,4*nrow)\n",
    "\n",
    "    fig, ax = plt.subplots(nrow, ncol, figsize=figsize, sharex=True, sharey=True)\n",
    "    plt.subplots_adjust(wspace = 0.1, hspace= 0.1)\n",
    "    \n",
    "    \n",
    "    cols = pd.MultiIndex.from_product([(\"prob\",\"max\"),[\"A\"]],names=[\"\",\"cluster letter\"])\n",
    "    qtable = pd.DataFrame(columns=cols)\n",
    "\n",
    "\n",
    "    for station in ds.station:\n",
    "        station_id = str(station.values)\n",
    "        if clean and station_id not in datums.keys(): # for alex's paper\n",
    "            if debug:\n",
    "                print(\"skipping \"+station_id, end=\" \")\n",
    "            continue\n",
    "        \n",
    "        qtable.loc[station_id,\"name\"] = datums[station_id]['name']\n",
    "        \n",
    "        # station latitude and longitude\n",
    "        slon = ds.x.sel(station=station).values[0] # just use first one; they are all the same\n",
    "        slat = ds.y.sel(station=station).values[0]\n",
    "\n",
    "        if ((slon < lonmin) | (slon > lonmax) | (slat < latmin) | (slat > latmax)):\n",
    "            if debug:\n",
    "                print(station_id+ ' out of bounds')\n",
    "            continue\n",
    "\n",
    "        datumObs = 'NAVD'\n",
    "        datumObs = 'MHHW'\n",
    "\n",
    "        df = pd.read_csv(turl(station_id,begin_date,end_date,datumObs))\n",
    "        if 'Date Time' not in list(df.keys()):\n",
    "            print(df, turl(station_id,begin_date,end_date,datumObs))\n",
    "            print('failed to read', station_id, begin_date, end_date, datumObs,'trying MSL')\n",
    "            datumObs = 'MSL'\n",
    "            df = pd.read_csv(turl(station_id,begin_date,end_date,datumObs))\n",
    "            if 'Date Time' not in list(df.keys()):\n",
    "                print(df)\n",
    "                print('failed to read', station_id, begin_date, end_date, datumObs, 'skipping')\n",
    "                continue\n",
    "\n",
    "        \n",
    "        # strip left and right whitespace from column names\n",
    "        df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "\n",
    "        if isinstance(df['Date Time'][0], str):\n",
    "            msg = df['Date Time'][0]\n",
    "            if 'Error:' in msg:\n",
    "                print(msg, \"skipping\", station_id)\n",
    "                continue\n",
    "\n",
    "                \n",
    "        # Make sure df['Date Time'][0] is not a string first\n",
    "        df['Date Time'] = pd.to_datetime(df['Date Time'])\n",
    "\n",
    "\n",
    "                \n",
    "        waterLevel = df['Water Level']\n",
    "        if not any(waterLevel):\n",
    "            print(\"no water levels\", df)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            obs_times = df['Date Time']\n",
    "        except:\n",
    "            set_trace()\n",
    "\n",
    "        print(\"plotting\", datums[station_id]['name'], station_id)\n",
    "\n",
    "        for iax, (cluster, cdict) in enumerate(clusters.items()):\n",
    "\n",
    "            if clean:\n",
    "                tsaxes = ax.flatten()[iax]\n",
    "                mapax = None\n",
    "            else:\n",
    "                tsaxes = ax[iax,0]\n",
    "                mapax = ax[iax,1]\n",
    "            # Plot ADCIRC time series\n",
    "            tsaxes.set_ylabel(ds.zeta.long_name+\"\\n\"+ds.zeta.units)\n",
    "            nice_grid(tsaxes)\n",
    "            tsaxes.set_ylim([-3.4,3.8]) \n",
    "\n",
    "            color = cdict['color']\n",
    "            cluster_letter = cdict['letter']\n",
    "            cluster_members = cdict['members']\n",
    "            cluster_members = [x.zfill(2) for x in cluster_members]\n",
    "            print(cluster_letter, ' color=',color, 'cluster members=',cluster_members)\n",
    "\n",
    "            # let xarray.plot take care of legend. it adds title 'member' and member labels\n",
    "            cluster_zeta = ds.zeta.sel(station = station, member=cluster_members)\n",
    "            # convert model output from NAVD88 to MHHW\n",
    "            cluster_zeta = to_MHHW(cluster_zeta, debug=debug)\n",
    "            mm = cluster_zeta.values.max(axis=1)\n",
    "            n = 0.\n",
    "            above1 = 0.\n",
    "            for m in mm:\n",
    "                if not np.isnan(m):\n",
    "                    if m >= 1:\n",
    "                        above1 = above1 + 1\n",
    "                    n = n+1\n",
    "            print(cluster_zeta.values.max(axis=1))\n",
    "\n",
    "            prob = above1/n if n>0 else 0\n",
    "            qtable.loc[station_id,(\"prob\",cluster_letter)] = '{:.0f}%'.format(np.round(100*prob))\n",
    "            qtable.loc[station_id,(\"max\",cluster_letter)] = '{:.2f}'.format(cluster_zeta.max().values) # don't need axis=1 here\n",
    "\n",
    "            m2d = cluster_zeta.plot.line(ax=tsaxes,x='time', color=color,add_legend = not clean,\n",
    "                                                            linewidth=0.8, label=member)\n",
    "            \n",
    "            top_of_lines = cluster_zeta.max(dim='time')\n",
    "            times_of_max = cluster_zeta.time[cluster_zeta.argmax(dim='time')]\n",
    "            if debug: print(times_of_max.values,top_of_lines.values)\n",
    "            label, = tsaxes.plot(times_of_max, top_of_lines, 'k*')\n",
    "\n",
    "            latlon_str = '({0:.5f}W,{1:.5f}N)'.format(-slon,slat)\n",
    "            \n",
    "            cluster_id = ' '.join([\n",
    "                init_date,\n",
    "                cluster_start_time,\n",
    "                str(cluster_deg),\n",
    "                str(ncluster),\n",
    "                str(cluster)\n",
    "            ])\n",
    "            \n",
    "            if mapax:\n",
    "                if debug: print(\"Drawing map\")\n",
    "                m = map_points(mapax, slon, slat, scalebuffer=2.5, debug=debug)    \n",
    "\n",
    "                if debug: print(\"setting map axis title\")\n",
    "                mapax.set_title('cluster'+cluster_id)\n",
    "\n",
    "                # Draw tracks\n",
    "                for member in cluster_members:\n",
    "                    trackfile = thisstorm.basedir+'WRF.2017090512.EPS_'+member+'.27km3km/fort.22'\n",
    "                    track = map_track(m, trackfile, color=color, debug=debug)\n",
    "                    if debug: print(\"added \"+trackfile+\" to map\")\n",
    "\n",
    "            if not clean:\n",
    "                if debug: print(\"setting time series axes title\")\n",
    "                tsaxes.set_title(station_id + ' ' + latlon_str + ' observed ('+datumObs+')')\n",
    "            else:\n",
    "                panel_letter = chr(iax+97)\n",
    "                if cdict['letter'].upper() != panel_letter.upper():\n",
    "                    print(\"cluster letter doesn't match panel letter\")\n",
    "                    pdb.set_trace()\n",
    "                panel_str = '('+panel_letter+')'\n",
    "                tsaxes.annotate(panel_str, (0.01, 0.98), xycoords='axes fraction',\n",
    "                           fontsize=28, verticalalignment='top')\n",
    "                \n",
    "            verified = df['Quality'] == 'v'\n",
    "            preliminary = df['Quality'] == 'p'\n",
    "\n",
    "            # TODO: add label but don't mess up member legend\n",
    "            if debug: print(\"plotting verified water height\")\n",
    "            o,  = tsaxes.plot(obs_times[verified],waterLevel[verified], color='black', linewidth=2, label='observed ('+datumObs+')')\n",
    "            op, = tsaxes.plot(obs_times[preliminary],waterLevel[preliminary], color=o.get_color(), alpha=0.5,label='preliminary')\n",
    "\n",
    "            if debug:\n",
    "                print(\"formatting xaxis\")\n",
    "            right=dt.datetime.strptime(end_date, '%Y%m%d')\n",
    "            if not clean:\n",
    "                # room for legend\n",
    "                right = right - dt.timedelta(hours=12)\n",
    "            tsaxes.set_xlim(left=dt.datetime.strptime(begin_date,'%Y%m%d'), right=right)\n",
    "        \n",
    "        for i in ax.flat:\n",
    "            #i.label_outer() # for some reason this prevents the last column x-axis labels from being printed if it is not the bottom row\n",
    "            i.set_title('') # hack - remove title\n",
    "            i.set_xlabel('')\n",
    "            \n",
    "        # Hide axes we don't need\n",
    "        if clean and ncluster % 2 != 0:\n",
    "            if debug:\n",
    "                print('odd number. hiding last axes')\n",
    "            ax[-1,-1].axis('off')\n",
    "            # restore xtick labels ; TODO FIX - label not restored after 1st station plot\n",
    "            #set_trace()\n",
    "            ax[1,1].xaxis.set_tick_params(labelbottom=True)\n",
    "            #ax[1,1].xaxis.set_ticklabels(ax[-1,-2].xaxis.get_ticklabels())\n",
    "\n",
    "        ofile = os.path.dirname(fort61[0])+'/'+station_id+'.png'\n",
    "        if do_clusters:\n",
    "            ofile = os.path.dirname(fort61[0])+'/cluster'+cluster_id.replace(' ','_')+'.'+station_id+'.png'\n",
    "\n",
    "        if debug: print(\"saving \"+ofile)\n",
    "        status = mysavfig(ofile, string = fort61[0], dpi=125 )\n",
    "        for i in ax.flat:\n",
    "            i.clear()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtable[[\"name\",\"prob\",\"max\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"{:.2f}\".cluster_zeta.values.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_zeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(thisstorm.basedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare control to USGS Sensors (IKE only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False and thisstorm.name == 'IKE':\n",
    "    # USGS stations in Open-File Report 2008-1365\n",
    "    # Monitoring Inland Storm Surge and Flooding from Hurricane Ike in Texas and Louisiana, September 2008\n",
    "    # By Jeffery W. East, Michael J. Turco, and Robert R. Mason, Jr.\n",
    "    stations =[\n",
    "    [\"SSS-TX-BRA-001\", \"Brazoria\", 29.21194, -95.20833, \"surge\"],\n",
    "    [\"SSS-TX-BRA-002\", \"Brazoria\", 29.08472, -95.28806, \"surge\"],\n",
    "    [\"SSS-TX-BRA-004\", \"Brazoria\", 28.86833, -95.44861, \"surge\"],\n",
    "    [\"SSS-TX-BRA-005\", \"Brazoria\", 28.94944, -95.55556, \"riverine\"],\n",
    "    [\"SSS-TX-BRA-006\", \"Brazoria\", 28.86667, -95.58722, \"surge\"],\n",
    "    [\"SSS-TX-BRA-007\", \"Brazoria\", 29.28667, -95.13139, \"riverine\"],\n",
    "    [\"SSS-TX-BRA-008\", \"Brazoria\", 29.03556, -95.39889, \"surge\"],\n",
    "    [\"SSS-TX-BRA-009\", \"Brazoria\", 29.01306, -95.32972, \"surge\"],\n",
    "    [\"SSS-TX-BRA-010\", \"Brazoria\", 29.33639, -95.28417, \"riverine\"],\n",
    "    [\"SSS-TX-BRA-011\", \"Brazoria\", 29.29667, -95.35667, \"riverine\"],\n",
    "    [\"SSS-TX-CAL-001\", \"Calhoun\", 28.40639, -96.71167, \"surge\"],\n",
    "    [\"SSS-TX-CAL-002\", \"Calhoun\", 28.44444, -96.40250, \"surge\"],\n",
    "    [\"SSS-TX-CAL-003\", \"Calhoun\", 28.61917, -96.61972, \"surge\"],\n",
    "    [\"SSS-TX-CAL-004\", \"Calhoun\", 28.66056, -96.41167, \"surge\"],\n",
    "    [\"SSS-TX-CAL-005\", \"Calhoun\", 28.64139, -96.32333, \"surge\"],\n",
    "    [\"SSS-TX-CHA-003\", \"Chambers\", 29.60417, -94.67528, \"surge\"],\n",
    "    [\"SSS-TX-CHA-004\", \"Chambers\", 29.77278, -94.68694, \"surge\"],\n",
    "    [\"SSS-TX-GAL-001\", \"Galveston\", 29.45139, -94.63417, \"beach/wave\"],\n",
    "    [\"SSS-TX-GAL-002\", \"Galveston\", 29.46583, -94.64806, \"surge\"],\n",
    "    [\"SSS-TX-GAL-005\", \"Galveston\", 29.59444, -94.39028, \"surge\"],\n",
    "    [\"SSS-TX-GAL-008\", \"Galveston\", 29.33444, -94.75111, \"beach/wave\"],\n",
    "    [\"SSS-TX-GAL-010\", \"Galveston\", 29.23806, -94.87778, \"beach/wave\"],\n",
    "    [\"SSS-TX-GAL-011\", \"Galveston\", 29.22083, -94.94472, \"surge\"],\n",
    "    [\"SSS-TX-GAL-015\", \"Galveston\", 29.08611, -95.11722, \"beach/wave\"],\n",
    "    [\"SSS-TX-GAL-016\", \"Galveston\", 29.30389, -94.90528, \"surge\"],\n",
    "    [\"SSS-TX-GAL-018\", \"Galveston\", 29.35583, -95.04000, \"surge\"],\n",
    "    [\"SSS-TX-GAL-019\", \"Galveston\", 29.50639, -94.95778, \"surge\"],\n",
    "    [\"SSS-TX-GAL-020\", \"Galveston\", 29.45667, -95.04778, \"riverine\"],\n",
    "    [\"SSS-TX-GAL-021\", \"Galveston\", 29.51333, -95.10389, \"riverine\"],\n",
    "    [\"SSS-TX-GAL-022\", \"Galveston\", 29.55167, -95.02472, \"surge\"],\n",
    "    [\"SSS-TX-HAR-002\", \"Harris\", 29.62028, -94.99889, \"surge_sensor_psige\"],\n",
    "    [\"SSS-TX-HAR-003\", \"Harris\", 29.59194, -95.12833, \"surge\"],\n",
    "    [\"SSS-TX-HAR-004\", \"Harris\", 29.71306, -94.99333, \"surge\"],\n",
    "    [\"SSS-TX-JEF-001\", \"Jefferson\", 29.68444, -94.19278, \"surge\"],\n",
    "    [\"SSS-TX-JEF-002\", \"Jefferson\", 29.67500, -94.04361, \"beach/wave\"],\n",
    "    [\"SSS-TX-JEF-004\", \"Jefferson\", 29.71028, -94.11639, \"surge\"],\n",
    "    [\"SSS-TX-JEF-005\", \"Jefferson\", 29.69694, -94.09833, \"surge\"],\n",
    "    [\"SSS-TX-JEF-006\", \"Jefferson\", 29.71111, -93.86000, \"surge\"],\n",
    "    [\"SSS-TX-JEF-007\", \"Jefferson\", 29.77389, -93.94250, \"surge\"],\n",
    "    [\"SSS-TX-JEF-008\", \"Jefferson\", 29.76472, -93.89778, \"surge\"],\n",
    "    [\"SSS-TX-JEF-009\", \"Jefferson\", 29.66265, -94.08835, \"beach/wave\"],\n",
    "    [\"SSS-TX-MAT-001\", \"Matagorda\", 28.72056, -96.27389, \"surge\"],\n",
    "    [\"SSS-TX-MAT-002\", \"Matagorda\", 28.78639, -96.15028, \"surge\"],\n",
    "    [\"SSS-TX-MAT-003\", \"Matagorda\", 28.78750, -95.99583, \"riverine\"],\n",
    "    [\"SSS-TX-MAT-004\", \"Matagorda\", 28.83889, -95.85278, \"riverine\"],\n",
    "    [\"SSS-TX-MAT-005\", \"Matagorda\", 28.60056, -95.97806, \"beach/wave\"],\n",
    "    [\"SSS-TX-MAT-006\", \"Matagorda\", 28.68306, -95.97556, \"riverine\"],\n",
    "    [\"SSS-TX-MAT-007\", \"Matagorda\", 28.61139, -96.21528, \"surge\"],\n",
    "    [\"SSS-TX-MAT-008\", \"Matagorda\", 28.76417, -95.62694, \"beach/wave\"],\n",
    "    [\"SSS-TX-MAT-009\", \"Matagorda\", 28.77056, -95.61667, \"surge\"],\n",
    "    [\"SSS-TX-MAT-010\", \"Matagorda\", 28.83639, -95.66833, \"riverine\"],\n",
    "    [\"SSS-LA-CAM-001\", \"Cameron\", 29.75028, -93.66361, \"surge\"],\n",
    "    [\"SSS-LA-CAM-002\", \"Cameron\", 29.76194, -93.58250, \"surge\"],\n",
    "    [\"SSS-LA-CAM-003\", \"Cameron\", 29.80417, -93.34889, \"surge\"],\n",
    "    [\"SSS-LA-CAM-010\", \"Cameron\", 29.78611, -93.11500, \"surge\"],\n",
    "    [\"SSS-LA-CAM-011\", \"Cameron\", 29.87056, -93.07972, \"surge\"],\n",
    "    [\"SSS-LA-CAM-012\", \"Cameron\", 29.77056, -93.01444, \"surge\"],\n",
    "    [\"SSS-LA-VER-006\", \"Vermillion\", 29.64111, -92.42694, \"surge\"],\n",
    "    [\"SSS-LA-VER-007\", \"Vermillion\", 29.60028, -92.34167, \"surge\"]\n",
    "    ]\n",
    "\n",
    "\n",
    "    nc_file = thisstorm.basedir+'control/fort.63.nc'\n",
    "    print(nc_file)\n",
    "    fh = Dataset(nc_file, mode='r')\n",
    "    meshlat = fh.variables['y'][:]\n",
    "    meshlon = fh.variables['x'][:]\n",
    "    mtime   = fh.variables['time']\n",
    "    zeta    = fh.variables['zeta'][:]\n",
    "    base_date = dt.datetime.strptime(mtime.base_date, \"%Y-%m-%d %H:%M:%S %Z\")\n",
    "    dt_time = [base_date+dt.timedelta(0,t) for t in mtime]\n",
    "\n",
    "    zeta_units = fh.variables['zeta'].units\n",
    "    long_name = fh.variables['zeta'].long_name\n",
    "    fh.close()\n",
    "\n",
    "    # assign units to zeta\n",
    "    zeta = units(zeta_units) * zeta\n",
    "    \n",
    "    # Convert to feet\n",
    "    zeta = zeta.to('feet')\n",
    "    \n",
    "    for station in stations:\n",
    "        \n",
    "        site = station[0]\n",
    "        lat = station[2]\n",
    "        lon = station[3]\n",
    "        gtype = station[4]\n",
    "        # Find closest node in fort.63.nc\n",
    "        dlat = meshlat - lat\n",
    "        dlon = meshlon - lon\n",
    "        dist = np.sqrt(dlat**2+dlon**2)\n",
    "        i = np.argmin(dist)\n",
    "        y = zeta[:,i] * sf\n",
    "        if y.any():\n",
    "            fig,ax = plt.subplots(1,2,figsize=(16,4))\n",
    "            ax[0].set_ylabel(long_name+\"\\n\"+zeta.units)\n",
    "            ax[0].grid()\n",
    "            ax[0].plot(dt_time,y,label='ADCIRC')\n",
    "            ax[0].set_title(site+\"(\"+gtype+\")\")\n",
    "            ax[0].set_xlim(left=dt.datetime(2008,9,11,12),right=dt.datetime(2008,9,14,6))\n",
    "            ax[0].xaxis.set_major_formatter(dates.DateFormatter('%-m/%-d %-H'+'Z'))\n",
    "            ax[0].xaxis.set_major_locator(dates.HourLocator(byhour=[0,12]))\n",
    "            fig.autofmt_xdate()\n",
    "            ax[0].set_xlabel(base_date.strftime(\"%Y\"))\n",
    "\n",
    "            link = \"http://pubs.usgs.gov/of/2008/1365/downloads/ike_\"+site+\".txt\"\n",
    "            obs = np.genfromtxt(link,names=True,dtype=None,skip_header=28)\n",
    "            obs_times = []\n",
    "            elevations = []\n",
    "            for o in obs:\n",
    "                (date, time, elevation, surge_sensor_psi, nearest_barometric_sensor_psi, temp_from_surge_sensor,\n",
    "                temp_from_barometric_sensor, bad_psi_from_surge_sensor,bad_psi_from_barometric_sensor,\n",
    "                bad_temp_from_surge_sensor, bad_temp_from_barometric_sensor) = o\n",
    "                obs_times.append(dt.datetime.strptime(date+time, \"%m-%d-%Y%H:%M\")+dt.timedelta(hours=5)) # CDT time zone\n",
    "                elevations.append(elevation)\n",
    "            ax[0].plot(obs_times,elevations,label='observed')\n",
    "            ax[0].legend(loc='best')\n",
    "            map_points(ax[1],meshlon[i],meshlat[i], marker='o',scalebuffer=0.15,resolution='h')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Perturbations Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# thisstorm must be defined earlier.\n",
    "\n",
    "kwdict = {\n",
    "    \"Ithresh\":'1.00m',\n",
    "    \"minus_astronomical_tide\": False,\n",
    "    \"dryland\" : 'MHHW',\n",
    "    \"debug\"   : True\n",
    "}\n",
    "\n",
    "if thisstorm.name == 'HARVEY' and kwdict['minus_astronomical_tide']:\n",
    "    print(\"Not interested in stubtracting astronomical tide from Harvey\")\n",
    "    sys.exit(1)\n",
    "    \n",
    "\n",
    "#  veer perturbations\n",
    "veers = Perturbations(thisstorm, ptype='veers', units='km',xlabel='Distance from control at landfall', **kwdict)\n",
    "\n",
    "#  speed perturbations\n",
    "speeds = Perturbations(thisstorm, ptype='speeds',units='km',xlabel=\"Distance from control at landfall\", **kwdict)\n",
    "\n",
    "#  vmax perturbations\n",
    "vmaxes = Perturbations(thisstorm, ptype='vmaxes',units=\"kts\",xlabel = \"Intensity change at landfall\", **kwdict)\n",
    "\n",
    "#  rmax perturbations\n",
    "rmaxes = Perturbations(thisstorm, ptype='rmaxes',units=\"km\",xlabel=\"Change in max. radius of 34-kt wind at landfall\", **kwdict)\n",
    "\n",
    "\n",
    "Perturbations_list = [veers, speeds, rmaxes, vmaxes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veers.area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series of Inundation Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(sharey=True,sharex=True,ncols=2,nrows=2,figsize=(10,7))\n",
    "\n",
    "for ax, perturbations in zip(axs.flatten(), Perturbations_list):\n",
    "    if len(perturbations) == 0:\n",
    "        print(\"no values for \"+perturbations.ptype+\" perturbations\")\n",
    "        ax.axis('off')\n",
    "        continue\n",
    "\n",
    "    ax.set_title(perturbations.ptype)\n",
    "    for pert in perturbations:\n",
    "        line, = ax.plot(pert.time,pert.inund,label=pert.valuelabel,markersize=3)\n",
    "        if pert.value == 0:\n",
    "            line.set_linewidth(3)\n",
    "            line.set_color('k')\n",
    "        if pert.value < 0:\n",
    "            line.set_marker('^')\n",
    "            line.set_markevery(3)\n",
    "\n",
    "\n",
    "    units = 'inundation volume ($\\mathregular{km^3}$)'\n",
    "    ax.set_ylabel(units)\n",
    "\n",
    "    ax.grid()\n",
    "\n",
    "    ax.xaxis.set_major_formatter(dates.DateFormatter('%-m/%-d %-H'+'Z'))\n",
    "    ax.xaxis.set_major_locator(dates.HourLocator(byhour=[0,6,12,18]))\n",
    "    fig.autofmt_xdate()\n",
    "    ax.set_xlabel(perturbations.time[0][0].strftime(\"%Y\"))\n",
    "    \n",
    "    # put legend\n",
    "    ax.legend(loc='best', ncol=2, fontsize=8)\n",
    "\n",
    "plt.suptitle(thisstorm.name)\n",
    "\n",
    "status = mysavfig(perturbations.oprefix() + '_timeseries.png', string = perturbations.desc, **savfig_dict )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thisstorm.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum water level percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if thisstorm.name != 'IRMA':\n",
    "    # WARNING\n",
    "    # THIS DOES NOT ISOLATE REGIONS LIKE FLORIDASW\n",
    "    # IT STILL COVERS THE WHOLE LAT/LON BOX\n",
    "\n",
    "    #  I SHOULD SWITCH TO perfect model file instead of looking for it here?\n",
    "    # The perfect model file has the \"regional\" filter applied.\n",
    "\n",
    "    # Don't assume control is called 'control'. \n",
    "    # It is more like 'nws19.control_newtides_2017090812'\n",
    "    i0, = np.where(veers.values == 0)\n",
    "    control_dir = veers.dirnames[i0[0]]\n",
    "    fh = Dataset(control_dir + '/maxele.63.nc', mode='r')\n",
    "    meshlon = fh.variables['x'][:]\n",
    "    meshlat = fh.variables['y'][:]\n",
    "    fh.close()\n",
    "    ibox = (meshlon >= lonmin) & (meshlon <= lonmax) & (meshlat >= latmin) & (meshlat <= latmax)\n",
    "    meshlon = meshlon[ibox]\n",
    "    meshlat = meshlat[ibox]\n",
    "    for perturbations in Perturbations_list:\n",
    "\n",
    "        if len(perturbations) == 0:\n",
    "            print(\"no values for \"+perturbations.ptype+\" perturbations\")\n",
    "            continue\n",
    "        fig, ax = plt.subplots(1,2,figsize=(12,4))\n",
    "        ps = np.array([95,99,99.9,100])\n",
    "        for p in ps:\n",
    "            maxele = []\n",
    "            for pert in perturbations:\n",
    "                maxele.append(np.percentile(pert.maxele[ibox],p))\n",
    "            # Used to plot perturbation.values on x-axis. Changed Feb 28, 2017.\n",
    "            line, = ax[0].plot(perturbations.get_xticks(),maxele,marker='o',label='%.1f'%p)\n",
    "        annotate_pts(perturbations, line)\n",
    "\n",
    "        imaxs = []\n",
    "        for pert in perturbations:\n",
    "            t = np.argmax(pert.maxele[ibox])\n",
    "            imaxs.append(t)\n",
    "\n",
    "        ax[0].set_ylabel('max. water level (m)')\n",
    "        ax[0].set_title(\"Percentiles of Max. Water Level for \"+thisstorm.name+\"\\n\"+thisstorm.domain)\n",
    "        ax[0].yaxis.grid()\n",
    "        ax[0].set_ylim(0,12)\n",
    "        legend = ax[0].legend(loc='best', fontsize=12)\n",
    "        ax[0].set_xlabel(perturbations.xlabel)\n",
    "        zeroLine(line)\n",
    "        forecast_err_boxes(ax[0],perturbations)\n",
    "\n",
    "        m = map_points(ax[1], meshlon[imaxs], meshlat[imaxs], marker='o', colors=line.get_color(), \n",
    "                       labels=perturbations.valuelabels)\n",
    "        ax[1].set_title('Locations of Max. Water Level for\\n'+perturbations.xlabel)\n",
    "\n",
    "        status = mysavfig(perturbations.oprefix(ptype=True)+ '_maxele.png',string=perturbations.desc, **savfig_dict)\n",
    "\n",
    "\n",
    "\n",
    "# Why do I get \"double\" veer (track) lines,?\n",
    "\n",
    "# maybe you haven't symbolically linked \"control\"\n",
    "# directory to \"track+0\",  \"veer+0\", \"vmax+0\",  \"speed+0\", etc.\n",
    "# No. That did not fix it. I didn't negate distance errors for negative veers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://drive.google.com/open?id=1n58sZmEkW0J1YR9DRvasOQwQpp2Ojwvm0fI3VBQTk_k\">Google Doc about length scale</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(sharey=True,ncols=2,nrows=2,figsize=(10,7))\n",
    "\n",
    "for ax, perturbations in zip(axs.flatten(), Perturbations_list):\n",
    "    if len(perturbations) == 0:\n",
    "        print(\"no values for \"+perturbations.ptype+\" perturbations\")\n",
    "        ax.axis(\"off\")\n",
    "        continue\n",
    "\n",
    "    thisline, = ax.plot(perturbations.get_xticks(),perturbations.length_scale,marker='o')\n",
    "    annotate_pts(perturbations, thisline)\n",
    "    ax.set_ylabel('length scale (km)')\n",
    "    ax.set_title(perturbations.ptype)\n",
    "    ax.yaxis.grid()\n",
    "    ax.set_xlabel(perturbations.xlabel)\n",
    "    zeroLine(thisline)\n",
    "    forecast_err_boxes(ax, perturbations)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(thisstorm.name)\n",
    "\n",
    "status = mysavfig(perturbations.oprefix()+'_length_scale.png',string=perturbations.desc, **savfig_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area of Inundation Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(sharey=True,ncols=2,nrows=2,figsize=(10,7))\n",
    "\n",
    "for ax, perturbations in zip(axs.flatten(), Perturbations_list):\n",
    "    if len(perturbations) == 0:\n",
    "        print(\"no values for \"+perturbations.ptype+\" perturbations\")\n",
    "        ax.axis(\"off\")\n",
    "        continue\n",
    "\n",
    "    # use comma after thisline to \"un-list\" the one-item list of line objects\n",
    "    thisline, = ax.plot(perturbations.get_xticks(),perturbations.area,marker='o')\n",
    "    annotate_pts(perturbations, thisline)\n",
    "    ax.set_ylabel('area of inundation zone\\n$\\mathregular{km^2}$')\n",
    "    ax.set_title(perturbations.ptype)\n",
    "    ax.yaxis.grid()\n",
    "    ax.set_xlabel(perturbations.xlabel)\n",
    "    zeroLine(thisline)\n",
    "    forecast_err_boxes(ax, perturbations)\n",
    "\n",
    "plt.suptitle(thisstorm.name)\n",
    "plt.tight_layout()\n",
    "\n",
    "status = mysavfig(perturbations.oprefix()+'_area.png',string=perturbations.desc, **savfig_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Depth in Inundation Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(sharey=True,ncols=2,nrows=2,figsize=(10,7))\n",
    "\n",
    "for ax, perturbations in zip(axs.flatten(),Perturbations_list):\n",
    "    if len(perturbations) == 0:\n",
    "        print(\"no values for \"+perturbations.ptype+\" perturbations\")\n",
    "        ax.axis('off')\n",
    "        continue\n",
    "\n",
    "    thisline, = ax.plot(perturbations.get_xticks(),perturbations.depth,marker='o')\n",
    "    ax.set_ylabel('avg. depth in inundation zone\\nm')\n",
    "    annotate_pts(perturbations, thisline)\n",
    "    ax.set_title(perturbations.ptype)\n",
    "    ax.yaxis.grid()\n",
    "    ax.set_xlabel(perturbations.xlabel)\n",
    "    zeroLine(thisline)\n",
    "    forecast_err_boxes(ax, perturbations)\n",
    "\n",
    "plt.suptitle(thisstorm.name)\n",
    "plt.tight_layout()\n",
    "\n",
    "status = mysavfig(perturbations.oprefix()+'_depth.png',string=perturbations.desc, **savfig_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storm following/Control zone Max. Inundation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(sharey=True,ncols=2,nrows=2,figsize=(10,7.))\n",
    "\n",
    "for ax, perturbations in zip(axs.flatten(), Perturbations_list):\n",
    "    if len(perturbations) == 0:\n",
    "        print(\"no values for \"+perturbations.ptype+\" perturbations\")\n",
    "        ax.axis(\"off\")\n",
    "        continue\n",
    "\n",
    "    storm_following_line, = ax.plot(perturbations.get_xticks(),perturbations.max_vol, marker='o',label='storm-following')\n",
    "    fixed_location_line, = ax.plot(perturbations.get_xticks(),perturbations.max_vol_in_ctrl, marker='o',label='fixed location')\n",
    "    if perturbations.next().fixed_time: # is this okay? calling next() to get one element from generator?\n",
    "        fixed_location_line.set_label(fixed_location_line.get_label() + ', fixed time')\n",
    "    ax.set_ylabel('inundation volume ($\\mathregular{km^3}$)')\n",
    "    ax.set_title(perturbations.ptype)\n",
    "    ax.set_xlabel(perturbations.xlabel)\n",
    "    zeroLine(storm_following_line,alpha=0.4)\n",
    "    zeroLine(fixed_location_line,alpha=0.4)\n",
    "    forecast_err_boxes(ax, perturbations)\n",
    "    ax.yaxis.grid()\n",
    "\n",
    "    \n",
    "    # Of the 2 lines, label the one with greater mean values    \n",
    "    if np.mean(perturbations.max_vol) > np.mean(perturbations.max_vol_in_ctrl):\n",
    "        annotate_pts(perturbations, storm_following_line)\n",
    "    else:\n",
    "        annotate_pts(perturbations, fixed_location_line)\n",
    "\n",
    "# put legend to the right of the current axis\n",
    "axs[0][0].legend(loc='best')\n",
    "plt.suptitle(thisstorm.name)\n",
    "plt.tight_layout()\n",
    "\n",
    "status = mysavfig(perturbations.oprefix()+'_maxvol.png',string=perturbations.desc, **savfig_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All storms on same plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = veers\n",
    "kwdict['xlabel'] = P.xlabel\n",
    "kwdict['ptype'] =  P.ptype\n",
    "kwdict['minus_astronomical_tide'] = False\n",
    "\n",
    "IKEp     = Perturbations(IKE,     **kwdict)\n",
    "CHARLEYp = Perturbations(CHARLEY, **kwdict)\n",
    "CHARIKEp = Perturbations(CHARIKE, **kwdict)\n",
    "IRMAp    = Perturbations(IRMA_SW,    **kwdict)\n",
    "HARVEYp  = Perturbations(HARVEY,  **kwdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "for perturbations in [IKEp, CHARLEYp, IRMAp, HARVEYp]:\n",
    "    x = perturbations.get_xticks()\n",
    "    ic, = np.where(np.isclose(x,0))\n",
    "    controly = perturbations.max_vol[ic[0]]\n",
    "    x = 1.+perturbations.values/100.\n",
    "    if perturbations.ptype == 'vmaxes':\n",
    "        norm = perturbations.storm.landfall['vmax']\n",
    "        x = perturbations.get_xticks()/norm\n",
    "    storm_following_line, = ax.plot(x,perturbations.max_vol/controly, \n",
    "                                    marker='o',label=perturbations.storm.name)\n",
    "    ax.set_ylabel('normalized inundation volume')\n",
    "    ax.set_title('storm-following')\n",
    "    ax.set_xlabel('normalized '+perturbations.ptype)\n",
    "\n",
    "ax.grid(True)\n",
    "# put legend to the right of the current axis\n",
    "ax.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "\n",
    "status = mysavfig('all_storms.'+perturbations.ptype+'.maxvol.png',string=perturbations.desc, **savfig_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All storms' r^2, Bias, Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "for perturbations in [IKEp, CHARLEYp, CHARIKEp, IRMAp, HARVEYp]:\n",
    "    if len(perturbations) == 0:\n",
    "        print(\"no values for \"+perturbations.ptype+\" perturbations\")\n",
    "        continue\n",
    "\n",
    "    r2 = []\n",
    "    bias = []\n",
    "    rmse = []\n",
    "\n",
    "    for pdir in perturbations.dirnames:\n",
    "        # for consistency I started outputting all file names with stride, even\n",
    "        # the perfect_cntl.ncl script. That is always stride01, hence the\n",
    "        # change here.\n",
    "        search_domain = re.sub(r'stride\\d\\d', 'stride01', perturbations.domain)\n",
    "        search_str = pdir + '/*minus_astronomical_tide' + str(perturbations.minus_astronomical_tide) +\\\n",
    "            '_' + perturbations.Ithresh + '.' + perturbations.dryland + '.' + search_domain +\\\n",
    "            '.perfectmodel.nc'\n",
    "        if len(glob.glob(search_str)) == 0:\n",
    "            print('Found no files matching', search_str)\n",
    "        for nc_file in glob.glob(search_str):\n",
    "\n",
    "            print('reading '+nc_file, end=' ')\n",
    "            fh = Dataset(nc_file, mode='r')\n",
    "            r2.append(fh.variables['r2'][:])\n",
    "            yave = np.mean(fh.variables['model'])\n",
    "            xave = np.mean(fh.variables['obs'])\n",
    "            bias.append(yave/xave)\n",
    "            obs = fh.variables['obs'][:] # tried np.asarray() but it removed _FillValue\n",
    "            model = fh.variables['model'][:]\n",
    "            rmse.append(np.sqrt(np.mean((obs - model)**2)))\n",
    "            fh.close()\n",
    "\n",
    "    print(perturbations.values, r2)\n",
    "    r2p, = ax.plot(perturbations.get_xticks(),r2,label=perturbations.storm.name,marker='o')\n",
    "\n",
    "ax.set_ylabel('Correlation Coefficient ($\\mathregular{r^2}$)')\n",
    "\n",
    "ax.set_xlabel(perturbations.xlabel)\n",
    "ax.grid(True)\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.suptitle(perturbations.ptype)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "status = mysavfig('all_storms.'+perturbations.ptype+'.stats.png',bbox_inches='tight',\n",
    "                      string=perturbations.desc, **savfig_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variability at a single point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbations = veers\n",
    "fig, ax = plt.subplots(1,2, figsize=(14,5))\n",
    "colors=[]\n",
    "lons=[]\n",
    "lats=[]\n",
    "nodes = [i['index'] for i in perturbations.pointA]\n",
    "As, i = np.unique(nodes, return_index=True)\n",
    "# convert list to numpy array to index with a slice of indices (i)\n",
    "for pointA in np.array(perturbations.pointA)[i]:\n",
    "    A = pointA['index']\n",
    "    lon = pointA['lon']\n",
    "    lat = pointA['lat']\n",
    "    a = []\n",
    "    for mxfile in perturbations.mxfile:\n",
    "        fh = Dataset(mxfile, mode='r')\n",
    "        zeta_max = fh.variables['zeta_max']\n",
    "        a.append(zeta_max[A])\n",
    "        fh.close()\n",
    "\n",
    "    line, = ax[0].plot(perturbations.get_xticks(), a, marker='x', markeredgewidth=2, label=str(A)+' (%.2fN ' % lat + '%.2fE)' % lon)\n",
    "    zeroLine(line)\n",
    "\n",
    "    colors.append(line.get_color())\n",
    "    lats.append(lat)\n",
    "    lons.append(lon)\n",
    "m = map_points(ax[1], lons, lats, marker='x', colors=colors, linewidth=2, scalebuffer=0.5)\n",
    "\n",
    "\n",
    "#ax[0].legend(loc='best',fontsize=8)\n",
    "ax[0].set_title(thisstorm.name)\n",
    "ax[0].set_xlabel(perturbations.xlabel)\n",
    "forecast_err_boxes(ax[0], perturbations)\n",
    "ax[0].yaxis.grid()\n",
    "ax[0].set_ylim(0, pointA['thresh'])\n",
    "ax[0].set_ylabel('max. inundation depth (m)')\n",
    "\n",
    "for dirname in veers.dirnames:\n",
    "    nc_file = dirname + '/fort.22'\n",
    "    track = map_track(nc_file)\n",
    "    # following lines moved to map_track() function\n",
    "    #lat0, lon0 = np.genfromtxt(nc_file,delimiter=',', usecols=(6,7),dtype=None, unpack=True)\n",
    "    #lat0 = [int(i[:-1])/10. for i in lat0]\n",
    "    #lon0 = [int(i[:-1])/ -10. for i in lon0]\n",
    "    #m.plot(lon0, lat0, 'black', latlon=True, label=nc_file[70:74],linewidth=(3 if 'control' in nc_file else .71))\n",
    "\n",
    "\n",
    "\n",
    "status = mysavfig(perturbations.oprefix(ptype=True)+'_pointA_maxvol.png',string=perturbations.desc, **savfig_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
