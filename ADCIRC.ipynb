{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show how storm perturbations affect storm surge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read multiple directories containing output from ADCIRC and produced by these NCL scripts\n",
    "<pre>\n",
    "/glade/p/work/ahijevyc/ADCIRC/bulge_timeseries.ncl  \n",
    "/glade/p/work/ahijevyc/ADCIRC/perfect_cntl.ncl\n",
    "</pre>\n",
    "These directories can hold different speed perturbations, veer perturbations, vmax, or rmax perturbations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed plots in web page. No floating window.\n",
    "%matplotlib inline\n",
    "# svg increases resolution when you zoom in (Ctrl-+); png does not.\n",
    "# Use svg format (scalable vector graphics) for plots in web page, not png\n",
    "%config InlineBackend.figure_formats=['png']\n",
    "timestamp = True # \"created by\" timestamp in lower-left corner\n",
    "dpi = 300\n",
    "\n",
    "savfig_dict = {\"dpi\":dpi, \"timestamp\": timestamp} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import dates\n",
    "import glob, sys, os, urllib, urllib2\n",
    "from netCDF4 import Dataset, chartostring\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.rcParams.update({'font.size': 12})\n",
    "#plt.rcParams.update({'lines.markersize': 8})\n",
    "from mysavfig import mysavfig\n",
    "\n",
    "# Allow the program to be stopped and debugged.\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Define Storm Class and Perturbation Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "\n",
    "class storm:\n",
    "    # Each storm is initiated with a name and a domain.\n",
    "    def __init__(self, name, domain):\n",
    "        self.name = name\n",
    "        # directory containing all the perturbation subdirectories.\n",
    "        self.basedir = '/glade/p/work/ahijevyc/ADCIRC/'+name+'/'\n",
    "        self.domain = domain\n",
    "\n",
    "\n",
    "class Perturbation:\n",
    "    # Called by Perturbations\n",
    "    def __init__(self, path, storm, Ithresh, minus_astronomical_tide, dryland):\n",
    "        j = os.path.basename(path)\n",
    "        n = max([j.find('+'), j.find('-')])\n",
    "        j = j[n:]\n",
    "        self.value = float(j)\n",
    "        self.path = path\n",
    "        \n",
    "        timeseries_search_str = path + \\\n",
    "            '/*.minus_astronomical_tide'+str(minus_astronomical_tide)+\\\n",
    "            '_'+Ithresh+'.'+dryland+'.'+storm.domain+'.timeseries.nc'\n",
    "\n",
    "        tsfile = glob.glob(timeseries_search_str)\n",
    "        if len(tsfile) != 1:\n",
    "            print \"Did not find one file matching \"+timeseries_search_str\n",
    "        \n",
    "        # Fill in these attributes of Perturbation object at time of max Inundation Volume:\n",
    "        # volume in control zone, length scale, inundation area, average depth\n",
    "        print tsfile[0]\n",
    "        fh = Dataset(tsfile[0], mode='r')\n",
    "        inund = fh.variables['inundation_volume'][:]\n",
    "        imax = np.argmax(inund)\n",
    "        \n",
    "        self.max_vol         = inund[imax]\n",
    "\n",
    "        # True = old way (get value at time of max inundation volume over whole domain)\n",
    "        self.fixed_time      = False\n",
    "        if self.fixed_time:\n",
    "            self.max_vol_in_ctrl = fh.variables['volume_in_ctrl'][imax]\n",
    "            self.length_scale    = fh.variables['length_scale'][imax]\n",
    "            self.area            = fh.variables['inundation_area'][imax]\n",
    "            self.depth           = fh.variables['average_depth'][imax]\n",
    "        else: \n",
    "            # new way (look for max value over all times, not just time of max inundation volume over\n",
    "            # whole domain)\n",
    "            # For example, the time with the most water in the ctrl zone is not necessarily the same\n",
    "            # as the time when the total inundation volume is greatest.\n",
    "            # Use [:]. or else you sometimes get _FillValue.\n",
    "            self.max_vol_in_ctrl = np.max(fh.variables['volume_in_ctrl'][:])\n",
    "            self.length_scale    = np.max(fh.variables['length_scale'][:])\n",
    "            self.area            = np.max(fh.variables['inundation_area'][:])\n",
    "            self.depth           = np.max(fh.variables['average_depth'][:])\n",
    "\n",
    "\n",
    "        self.inund           = inund\n",
    "        \n",
    "        # Ignore time zone (used to be \"UTC\" but now it is \"-0:00\") Had to manually\n",
    "        # adjust netcdf time attributes when base time is not 0Z.\n",
    "        # Fixed issues with adcirc starting at 18Z for Irma.\n",
    "        simple_dt = \" \".join(fh.variables['time'].base_date.split()[0:2])\n",
    "        base_date    = dt.datetime.strptime(simple_dt, \"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        self.time = [base_date + dt.timedelta(0,t) for t in fh.variables['time']]\n",
    "        fh.close()\n",
    "\n",
    "        self.valuelabel = '{0:+g}'.format(self.value)\n",
    "        #if self.value == 0:\n",
    "            #self.valuelabel = '' # Uncomment to not include control in legends\n",
    "\n",
    "        # Also get max water level field. Find node with highest inundation below 6.5 m\n",
    "        mxfile = path+'/maxele.63.nc'\n",
    "        Athresh = 6.5\n",
    "\n",
    "        try:\n",
    "            fh = Dataset(mxfile, mode='r')\n",
    "        except:\n",
    "            if not os.path.exists(mxfile):\n",
    "                print \"maxele.63.nc not in\", path\n",
    "            if os.path.islink(mxfile) and not os.path.exists(os.readlink(mxfile)):\n",
    "                print \"maxele.63.nc symbolic link broken in\", path\n",
    "            print \"could not read\", self.mxfile    \n",
    "            self.mxfile = None\n",
    "            maxele = None\n",
    "            A = None\n",
    "            value = None\n",
    "            lon = None\n",
    "            lat = None\n",
    "        else:\n",
    "            self.mxfile = mxfile\n",
    "            maxele = fh.variables['zeta_max'][:]\n",
    "            # Find a node (largest index) with max water height < 6.5m, south of 40°N\n",
    "            A = np.argmax(maxele * (maxele<Athresh) * (fh.variables['y'][:]<40))\n",
    "            lon = fh.variables['x'][A]\n",
    "            lat = fh.variables['y'][A]\n",
    "            value = maxele[A]\n",
    "\n",
    "        fh.close()\n",
    "\n",
    "        self.maxele = maxele\n",
    "        self.pointA = {\"index\":A, \"value\":value, \"thresh\":Athresh, \"lon\":lon, \"lat\":lat}\n",
    "        print path\n",
    "\n",
    "        \n",
    "class Perturbations:\n",
    "    \n",
    "    # Returns an ordered list of perturbation instances in increasing order. \n",
    "    \n",
    "    def __init__(self, storm, ptype, units=\"\", xlabel=\"\", Ithresh='1.00m', \n",
    "                 minus_astronomical_tide=False, dryland='MHHW'):\n",
    "\n",
    "        # Map perturbation type to subdirectory name.\n",
    "        if ptype == 'veers' :\n",
    "            pstr = 'veer'\n",
    "            if storm.name == 'HARVEY':\n",
    "                pstr = 'track'\n",
    "        elif ptype == 'speeds':\n",
    "            pstr = 'speed'\n",
    "        elif ptype == 'vmaxes':\n",
    "            pstr = 'vmax_PcAdjust'\n",
    "            if storm.name in ['IRMA','HARVEY'] :\n",
    "                pstr = 'vmax'\n",
    "        elif ptype == 'rmaxes':\n",
    "            pstr = 'rmax'\n",
    "        else:\n",
    "            print \"Unknown perturbation type\", name\n",
    "            sys.exit(1)\n",
    "            \n",
    "        if storm.name in ['IRMA','HARVEY']:\n",
    "            pstr = pstr + '_nws19'\n",
    "            \n",
    "        # find all the directories, scrape off numeric part, sort numerically\n",
    "        dirnames = glob.glob(storm.basedir+pstr+'[+-]*[0-9]')\n",
    "\n",
    "        self.storm   = storm\n",
    "        self.ptype   = ptype\n",
    "        self.units   = units\n",
    "        self.Ithresh = Ithresh\n",
    "\n",
    "        members = []\n",
    "        for i in dirnames:\n",
    "            members.append(Perturbation(i, storm, Ithresh, minus_astronomical_tide, dryland))\n",
    "            \n",
    "        self.members  = sorted(members, key=lambda x: x.value, reverse=False)\n",
    "        self.values          = [i.value for i in self.members]\n",
    "        self.dirnames        = [i.path for i in self.members]\n",
    "        self.area            = [i.area for i in self.members]\n",
    "        self.depth           = [i.depth for i in self.members]\n",
    "        self.domain          = storm.domain\n",
    "        self.inund           = [i.inund for i in self.members]\n",
    "        self.length_scale    = [i.length_scale for i in self.members]\n",
    "        self.maxele          = [i.maxele for i in self.members]\n",
    "        self.max_vol_in_ctrl = [i.max_vol_in_ctrl for i in self.members]\n",
    "        self.max_vol         = [i.max_vol for i in self.members]\n",
    "        self.mxfile          = [i.mxfile for i in self.members]\n",
    "        self.pointA          = [i.pointA for i in self.members]\n",
    "        self.time            = [i.time for i in self.members]\n",
    "        self.valuelabels     = [i.valuelabel for i in self.members]\n",
    "        self.xlabel = xlabel + ' (' + units + ')'\n",
    "        self.index = 0\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        self.values = np.array(self.values)\n",
    "\n",
    "        # Get desc from attributes of length_scale variable in control run.\n",
    "        ifile = storm.basedir + 'control/control.minus_astronomical_tide' + str(minus_astronomical_tide) + '_' + \\\n",
    "            Ithresh + '.' + dryland + '.' + storm.domain + '.timeseries.nc'\n",
    "        fh = Dataset(ifile, mode='r')\n",
    "        ls = fh.variables['length_scale']\n",
    "        self.desc = pstr + ' minus_astronomical_tide:' +str(minus_astronomical_tide) + \\\n",
    "            \" \" + dryland + '\\ninundation depth threshold: ' + \\\n",
    "            ls.depth_threshold +'\\nlength scale left side: ' + \\\n",
    "            str(ls.left_percentile) + '%; right: ' + str(ls.right_percentile) + '%'\n",
    "        fh.close()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.values)\n",
    "            \n",
    "    # Allow perturbations to be iterated over\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    def next(self):\n",
    "        if self.index < len(self.values):\n",
    "            member = self.members[self.index]\n",
    "            self.index += 1\n",
    "            return member\n",
    "        else:\n",
    "            self.index = 0\n",
    "            raise StopIteration\n",
    "    \n",
    "    def get_xticks(self):\n",
    "\n",
    "        # x-axis may be converted from perturbation magnitude to something else.\n",
    "\n",
    "        # Convert veer perturbations to cross-track distance right of landfall (km)\n",
    "        # Convert speed perturbations to along-track distance at landfall (km)\n",
    "        # Keep vmax perturbation (kts)\n",
    "        # Keep rmax perturbation (%)\n",
    "\n",
    "        stormname = self.storm.name\n",
    "\n",
    "        if self.ptype == 'veers':\n",
    "            if 'IKE' in stormname:\n",
    "                # From https://docs.google.com/a/ucar.edu/spreadsheets/d/1_Y9dX2240jMZ1abgZt4E6Td-pODq6k3q8rxVKwz55Us/edit?usp=sharing\n",
    "                pert,dist = zip(*[(-6,-250),(-5,-198),(-4,-157),(-3,-121),(-2,-97),(-1,-49),(-0.5,-24),(0.,0.),\n",
    "                                  (0.5,22),(1.0,44),(2.0,82),(3.0,135),(4.0,146),(5.0,170),(6.0,199)])\n",
    "            if stormname == 'CHARLEY':\n",
    "                # https://docs.google.com/a/ucar.edu/spreadsheets/d/1gKj72PWl8g_0QAOCMZawT2c8QRfDa1DreanUWU4DC74/edit?usp=sharing\n",
    "                # Spreadsheet lists these from +4.5 to -4.5\n",
    "                pert = [-4.5, -3.5, -2.5, -1.5, -0.5, 0, 0.5, 1.5, 2.5, 3.5, 4.5]\n",
    "                dist = [-343, -289, -153,  -63,  -15, 0,  32,  63, 102, 122, 134]\n",
    "            if stormname == 'IRMA':\n",
    "                # https://docs.google.com/a/ucar.edu/spreadsheets/d/1QtxUjO3qIXf2ySPRh0RPG51KasDUCQwmp4ac0hKG3zk/edit?usp=sharing\n",
    "                # Copied and pasted into vim window and edited to look like this.\n",
    "                pert,dist = zip(*[(-7,-230), (-6,-195), (-5,-181), (-4,-136), (-3,-98), (-2,-63), (-1,-35),\n",
    "                                  (0,0), (1, 30), (2,60), (3,90), (4,128), (5,158), (6,188), (7,217)])\n",
    "            if stormname == 'HARVEY':\n",
    "                # https://docs.google.com/spreadsheets/d/1TBdFfGUwdJ9T5Oh7Kv2T7thGvZXeqU1Q_Laj0ztgN4U/edit?usp=sharing\n",
    "                pert,dist = zip(*[(-7,-277.80), (-6,-255.58), (-5,-244.46), (-4,-118.53), (-3 ,-88.90),\n",
    "                                  (-2,-59.26), (-1,-29.63), (0,0.00), (1,31.48), (2,59.26), (3,96.30),\n",
    "                                  (4 ,118.53), (5 ,150.01), (6 ,177.79), (7 ,207.42)])\n",
    "\n",
    "\n",
    "        if self.ptype == 'speeds':\n",
    "            if 'IKE' in stormname:\n",
    "                # From https://docs.google.com/a/ucar.edu/spreadsheets/d/1_Y9dX2240jMZ1abgZt4E6Td-pODq6k3q8rxVKwz55Us/edit?usp=sharing\n",
    "                pert,dist = zip(*[(-20,-401),(-15,-273),(-10,-173),(-5,-61),(0.,0.),(5,43),(10,130),(15,248)])\n",
    "            if stormname == 'CHARLEY':\n",
    "                # From https://docs.google.com/a/ucar.edu/spreadsheets/d/1gKj72PWl8g_0QAOCMZawT2c8QRfDa1DreanUWU4DC74/edit?usp=sharing\n",
    "                pert = [ -15, -12.5,  -10, -7.5,   -5, -2.5, 0, 2.5,   5, 7.5,  10,  15]\n",
    "                dist = [-378,  -329, -260, -177, -116,  -35, 0,  94, 163, 220, 275, 348] # km\n",
    "            if stormname == 'IRMA':\n",
    "                # From https://docs.google.com/a/ucar.edu/spreadsheets/d/1QtxUjO3qIXf2ySPRh0RPG51KasDUCQwmp4ac0hKG3zk/edit?usp=sharing\n",
    "                # Copied and pasted into vim window and edited to look like this.\n",
    "                pert,dist = zip(*[(-20,-314),(-15,-239), (-10,-159), (-5,-67), (0,0), (5,75), (10,154), (15,200), (20,236)])\n",
    "            if stormname == 'HARVEY':\n",
    "                pert,dist = zip(*[(-20,-125.936),(-15,-88.896),(-10,-64.82),(-5,-25.928),(0,0),(5,61.116),(10,120.38),\n",
    "                                  (15,164.828),(20,237.056)])\n",
    "\n",
    "\n",
    "        if self.ptype == 'vmaxes':\n",
    "            pert,dist = zip(*[(-7,-28),(7,28)])\n",
    "\n",
    "\n",
    "        if self.ptype == 'rmaxes':\n",
    "            # Radius change of +100% error.\n",
    "            # Find landfall time in control/fort.22\n",
    "            # Find 34kt line. \n",
    "            # Look for max value in columns 35,36,37,38 (in nautical miles)\n",
    "            # Convert to km.\n",
    "            # IKE and CHARLEY from Kate Fossell's Feb 28 2017 email\n",
    "            if stormname ==    'IKE': rmax100 = 75.19 # km\n",
    "            if 'CHAR' in stormname:   rmax100 = 36.85 # km\n",
    "            if stormname ==   'IRMA': rmax100 = 60.93 # km\n",
    "            if stormname == 'HARVEY': rmax100 = 44.818 # km\n",
    "            pert,dist = zip(*[(-100.,-rmax100),(100.,rmax100)])\n",
    "\n",
    "        if np.min(pert) >= 0:\n",
    "            print \"no negative perturbations\", pert\n",
    "            print \"did you forget to negate the negative veers and speeds?\"\n",
    "            exit(2)\n",
    "\n",
    "        xticks = np.interp(self.values, pert, dist)\n",
    "        return xticks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create storm objects with domains (must have been created by NCL scripts already)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IKE     = storm(name='IKE',     domain='stride10.-99.0E-87.0E25.0N31.0N')\n",
    "CHARLEY = storm(name=\"CHARLEY\", domain='stride02.-86.0E-78.0E24.2N30.0N')\n",
    "CHARIKE = storm(name='CHARIKE', domain='stride10.-99.0E-87.0E25.0N31.0N')\n",
    "IRMA    = storm(name='IRMA',    domain='stride02.-88.0E-77.0E24.0N35.0NFloridaNE')\n",
    "HARVEY  = storm(name='HARVEY',  domain='stride02.-98.0E-92.2E24.8N30.5N')\n",
    "\n",
    "\n",
    "thisstorm = HARVEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define map_points() for Basemap plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.basemap import Basemap\n",
    "import cf_units # Provision of wrapper class to support UDUNITS-2, netcdftime calendar\n",
    "\n",
    "\n",
    "def map_points(ax, lon, lat, s=64, scalebuffer=1., colors='k', linewidth=0.5, resolution='i', \n",
    "               labels=None, **kwargs):\n",
    "    # Scatter plot on a map.\n",
    "    # Domain large enough to encompasses all the markers, plus a 3° longitude and 1° latitude buffer. \n",
    "    m = Basemap(lon_0=-90,lat_0=30,resolution=resolution,projection='stere',\\\n",
    "                llcrnrlat=np.amin(lat)-1.*scalebuffer,\\\n",
    "                urcrnrlat=np.amax(lat)+1.*scalebuffer,\\\n",
    "                llcrnrlon=np.amin(lon)-3.*scalebuffer,\\\n",
    "                urcrnrlon=np.amax(lon)+3.*scalebuffer,ax=ax)\n",
    "    m.drawcoastlines(linewidth=0.5)\n",
    "    m.drawmapboundary(fill_color='aqua')\n",
    "    # fill continents, set lake color same as ocean color.\n",
    "    m.fillcontinents(color='white',lake_color='aqua',zorder=0)\n",
    "    # draw parallels and meridians.\n",
    "    # label parallels on right and top\n",
    "    # meridians on bottom and left\n",
    "    parallels = np.arange(0.,81,1.)\n",
    "    # labels option turns off/on label. 4 element list for 4 sides: [left,right,top,bottom]\n",
    "    m.drawparallels(parallels,labels=[True,True,False,True])\n",
    "    meridians = np.arange(10.,351.,2.)\n",
    "    m.drawmeridians(meridians,labels=[True,True,False,True])\n",
    "    m.scatter(lon, lat, latlon=True, s=s, c=colors, linewidth=linewidth, **kwargs)\n",
    "    if labels:\n",
    "        for x, y, label in zip(lon, lat, labels):\n",
    "            ax.annotate(label, xy=m(x, y), ha='center', va='center', fontsize=5.)\n",
    "\n",
    "\n",
    "    return m\n",
    "\n",
    "def forecast_err_box(xmin, xmax, label, **kwargs):\n",
    "    # Called by forecast_err_boxes()\n",
    "    # Draw box from xmin to xmax, spanning the vertical.\n",
    "    # Label vertical lines too.\n",
    "    ax = kwargs['axes']\n",
    "    # Dont' change the xaxis range\n",
    "    ax.autoscale(False)\n",
    "    box = ax.axvspan(xmin, xmax, alpha=0.075, facecolor='red', **kwargs)\n",
    "    edgecolor = 'red'\n",
    "    box = ax.axvspan(xmin, xmax, alpha=0.2, facecolor=\"none\", edgecolor=edgecolor, lw=1.8, **kwargs)\n",
    "    labelL = ax.annotate(s=label, xy=(xmin,0.5), ha=\"center\", va=\"center\", xycoords=('data', 'axes fraction'), \n",
    "                         rotation=90, fontsize=12, annotation_clip=True, zorder=1, alpha=0.75, **kwargs)\n",
    "    labelR = ax.annotate(s=label, xy=(xmax,0.5), ha=\"center\", va=\"center\", xycoords=('data','axes fraction'), \n",
    "                         rotation=90, fontsize=12, annotation_clip=True, zorder=1, alpha=0.75, **kwargs)\n",
    "\n",
    "def forecast_err_boxes(ax, perturbation):\n",
    "    # Draw tranparent red boxes to illustrate typical forecast err for different lead times.\n",
    "\n",
    "    # clunky way to get nautical miles (multiply knots by hour)\n",
    "    nm = cf_units.Unit('knot') * cf_units.Unit('hour')\n",
    "\n",
    "    if perturbation.ptype == 'veers':\n",
    "        # list of (hour, nautical miles) tuples to define 2010-2014 cone from \n",
    "        # https://docs.google.com/a/ucar.edu/spreadsheets/d/1_Y9dX2240jMZ1abgZt4E6Td-pODq6k3q8rxVKwz55Us/edit?usp=sharing\n",
    "        fhr, radii = zip(*[(12,32),(24,52),(36,71),(48,90),(72,122),(96,170),(120,225)])\n",
    "        \n",
    "        # Convert nautical miles to km\n",
    "        radii = nm.convert(np.array(radii), cf_units.Unit('km'))\n",
    "        fhrboxes = [12,24,48,72]\n",
    "\n",
    "    if perturbation.ptype == 'speeds':\n",
    "        # list of (hour, km) tuples to define 2010-2014 cone estimated to be more than cross-track error from\n",
    "        # https://docs.google.com/a/ucar.edu/spreadsheets/d/1_Y9dX2240jMZ1abgZt4E6Td-pODq6k3q8rxVKwz55Us/edit?usp=sharing\n",
    "        fhr, radii = zip(*[(12,32),(24,52),(36,71),(48,90),(72,122),(96,170),(120,225)])\n",
    "        # Convert from nm to km\n",
    "        radii = nm.convert(np.array(radii), cf_units.Unit('km'))\n",
    "        fhrboxes = [12,24,48,72]\n",
    "\n",
    "    if perturbation.ptype == 'vmaxes':\n",
    "        # list of (hour, knots) tuples from NHC published stats \n",
    "        # https://drive.google.com/open?id=0B4GoIuq38OVyLTRhTkxmbE1YZVY1eU5xYlA3RlJWOHlyN3A0\n",
    "        fhr, radii = zip(*[(24,10),(48,15),(72,20)])\n",
    "        fhrboxes = fhr\n",
    "        \n",
    "    if perturbation.ptype == 'rmaxes':\n",
    "        # list of (hour, km) tuples Feb 10 2017 email from Kate Fossell\n",
    "        # Based on Cangialosi and Landsea (2016)\n",
    "        fhr, radii = zip(*[(12,37),(24,52),(36,60),(48,63),(72,68)])\n",
    "        fhrboxes = [12,24,48,72]\n",
    "        \n",
    "    try:\n",
    "        fhr\n",
    "    except NameError:\n",
    "        print \"no foreast lead time window for\", perturbation.name, perturbation.storm.name\n",
    "        return\n",
    "    # np.interp(x,xp,fp)\n",
    "    # x: x-coordinates of interpolated values\n",
    "    # xp: 1-D seq of floats. the x-coordinates of the data points\n",
    "    # fp: 1-D seq of floats or complex. The y-coordinates of the data points, same len as xp.\n",
    "    # Get errors assocated with fboxes list.\n",
    "    lefts = -np.interp(fhrboxes, fhr, radii)\n",
    "    rights = np.interp(fhrboxes, fhr, radii)\n",
    "    for box, left, right in zip(fhrboxes, lefts, rights):\n",
    "        junk = forecast_err_box(left, right, '%d-hr' % box, axes=ax)\n",
    "        \n",
    "def zeroLine(line, **kwargs):\n",
    "    line.axes.axvline(x=0, linestyle='dashed', color='black', zorder=1)\n",
    "    # Use np.isclose because of rmaxes, where zero tick is interpolated to a tiny non-zero value.\n",
    "    i0, = np.where(np.isclose(line.get_xdata(), 0))\n",
    "    line.axes.axhline(y=line.get_ydata()[i0], linestyle='dashed', color=line.get_color(), **kwargs)\n",
    "            \n",
    "def annotate_pts(perturbation, line):\n",
    "    for label, xypoints in zip(perturbation.valuelabels, line.get_xydata()):\n",
    "        line.axes.annotate(label, xy=xypoints, textcoords='offset points', xytext=(0,+4), \n",
    "                           ha='center', va='bottom', fontsize=6.5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from http://code.activestate.com/recipes/491261-caching-and-throttling-for-urllib2/ May 16, 2017\n",
    "import httplib\n",
    "import unittest\n",
    "import hashlib\n",
    "import StringIO\n",
    "__version__ = (0,1)\n",
    "__author__ = \"Staffan Malmgren <staffan@tomtebo.org>\"\n",
    "\n",
    "\n",
    "class CacheHandler(urllib2.BaseHandler):\n",
    "    \"\"\"Stores responses in a persistant on-disk cache.\n",
    "\n",
    "    If a subsequent GET request is made for the same URL, the stored\n",
    "    response is returned, saving time, resources and bandwith\"\"\"\n",
    "    def __init__(self,cacheLocation):\n",
    "        \"\"\"The location of the cache directory\"\"\"\n",
    "        self.cacheLocation = cacheLocation\n",
    "        if not os.path.exists(self.cacheLocation):\n",
    "            os.mkdir(self.cacheLocation)\n",
    "            \n",
    "    def default_open(self,request):\n",
    "        if ((request.get_method() == \"GET\") and \n",
    "            (CachedResponse.ExistsInCache(self.cacheLocation, request.get_full_url()))):\n",
    "            # print \"CacheHandler: Returning CACHED response for %s\" % request.get_full_url()\n",
    "            return CachedResponse(self.cacheLocation, request.get_full_url(), setCacheHeader=True)\n",
    "        else:\n",
    "            return urllib2.urlopen(request.get_full_url())\n",
    "\n",
    "    def http_response(self, request, response):\n",
    "        if request.get_method() == \"GET\":\n",
    "            if 'd-cache' not in response.info():\n",
    "                CachedResponse.StoreInCache(self.cacheLocation, request.get_full_url(), response)\n",
    "                return CachedResponse(self.cacheLocation, request.get_full_url(), setCacheHeader=False)\n",
    "            else:\n",
    "                return CachedResponse(self.cacheLocation, request.get_full_url(), setCacheHeader=True)\n",
    "        else:\n",
    "            return response\n",
    "    \n",
    "class CachedResponse(StringIO.StringIO):\n",
    "    \"\"\"An urllib2.response-like object for cached responses.\n",
    "\n",
    "    To determine wheter a response is cached or coming directly from\n",
    "    the network, check the x-cache header rather than the object type.\"\"\"\n",
    "    \n",
    "    def ExistsInCache(cacheLocation, url):\n",
    "        hash = hashlib.md5(url).hexdigest()\n",
    "        return (os.path.exists(cacheLocation + \"/\" + hash + \".headers\") and \n",
    "                os.path.exists(cacheLocation + \"/\" + hash + \".body\"))\n",
    "    ExistsInCache = staticmethod(ExistsInCache)\n",
    "\n",
    "    def StoreInCache(cacheLocation, url, response):\n",
    "        hash = hashlib.md5(url).hexdigest()\n",
    "        f = open(cacheLocation + \"/\" + hash + \".headers\", \"w\")\n",
    "        headers = str(response.info())\n",
    "        f.write(headers)\n",
    "        f.close()\n",
    "        f = open(cacheLocation + \"/\" + hash + \".body\", \"w\")\n",
    "        f.write(response.read())\n",
    "        f.close()\n",
    "    StoreInCache = staticmethod(StoreInCache)\n",
    "    \n",
    "    def __init__(self, cacheLocation,url,setCacheHeader=True):\n",
    "        self.cacheLocation = cacheLocation\n",
    "        hash = hashlib.md5(url).hexdigest()\n",
    "        StringIO.StringIO.__init__(self, file(self.cacheLocation + \"/\" + hash+\".body\").read())\n",
    "        self.url     = url\n",
    "        self.code    = 200\n",
    "        self.msg     = \"OK\"\n",
    "        headerbuf = file(self.cacheLocation + \"/\" + hash+\".headers\").read()\n",
    "        if setCacheHeader:\n",
    "            headerbuf += \"d-cache: %s/%s\\r\\n\" % (self.cacheLocation,hash)\n",
    "        self.headers = httplib.HTTPMessage(StringIO.StringIO(headerbuf))\n",
    "\n",
    "    def info(self):\n",
    "        return self.headers\n",
    "    def geturl(self):\n",
    "        return self.url\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Control to NOAA Stations (IKE only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turl(id,datum):\n",
    "    return \"http://tidesandcurrents.noaa.gov/api/datagetter?product=water_level&application=NOS.COOPS.TAC.WL&station=\"+\\\n",
    "        id+\"&begin_date=20080911&end_date=20080914&datum=\"+datum+\"&units=english&time_zone=GMT&format=csv\"\n",
    "\n",
    "if False and thisstorm.name == 'IKE':\n",
    "    fh = Dataset(thisstorm.basedir+'control/fort.61.nc', mode='r')\n",
    "    stations = chartostring(fh.variables['station_name'][:])\n",
    "    meshlon = fh.variables['x'][:]\n",
    "    meshlat = fh.variables['y'][:]\n",
    "    mtime = fh.variables['time']\n",
    "    base_date = dt.datetime.strptime(mtime.base_date, \"%Y-%m-%d %H:%M:%S %Z\")\n",
    "    dt_time = [base_date+dt.timedelta(0,t) for t in mtime]\n",
    "    fhzeta = fh.variables['zeta']\n",
    "    zeta = fhzeta[:]\n",
    "    units = cf_units.Unit(fhzeta.units)\n",
    "    long_name = fhzeta.long_name\n",
    "    fh.close()\n",
    "\n",
    "    # convert to feet\n",
    "    zeta = units.convert(zeta,cf_units.Unit('feet'))\n",
    "\n",
    "    id = stations[1].strip()\n",
    "    datum = 'NAVD'\n",
    "    reader = csv.DictReader(urllib2.urlopen(turl(id,datum)))\n",
    "    print dir(reader), reader.line_num\n",
    "\n",
    "    import urllib, csv, re\n",
    "    import cf_units # Provision of wrapper class to support UDUNITS-2, netcdftime calendar\n",
    "    \n",
    "    # NOAA station data come from tidesandcurrents.noaa.gov\n",
    "    def turl(id,datum):\n",
    "        return \"http://tidesandcurrents.noaa.gov/api/datagetter?product=water_level&application=NOS.COOPS.TAC.WL&station=\"+\\\n",
    "            id+\"&begin_date=20080911&end_date=20080914&datum=\"+datum+\"&units=english&time_zone=GMT&format=csv\"\n",
    "    \n",
    "    fh = Dataset(thisstorm.basedir+'control/fort.61.nc', mode='r')\n",
    "    stations = chartostring(fh.variables['station_name'][:])\n",
    "    meshlon = fh.variables['x'][:]\n",
    "    meshlat = fh.variables['y'][:]\n",
    "    mtime = fh.variables['time']\n",
    "    base_date = dt.datetime.strptime(mtime.base_date, \"%Y-%m-%d %H:%M:%S %Z\")\n",
    "    dt_time = [base_date+dt.timedelta(0,t) for t in mtime]\n",
    "    fhzeta = fh.variables['zeta']\n",
    "    zeta = fhzeta[:]\n",
    "    units = cf_units.Unit(fhzeta.units)\n",
    "    long_name = fhzeta.long_name\n",
    "    fh.close()\n",
    "\n",
    "    # convert to feet\n",
    "    zeta = units.convert(zeta,cf_units.Unit('feet'))\n",
    "\n",
    "    opener = urllib2.build_opener(CacheHandler(\".urllib2cache\"))\n",
    "\n",
    "\n",
    "    for i, station in enumerate(stations[0:]):\n",
    "        id = station.strip()\n",
    "        y = zeta[:,i]\n",
    "        if y.any():\n",
    "            print id, \"zeta_max:\", np.ma.max(y),\n",
    "\n",
    "            datum = 'NAVD'\n",
    "            nrow = sum(1 for row in csv.DictReader(opener.open(turl(id,datum))))\n",
    "            if nrow <= 1:\n",
    "                print 'no NAVD, trying MSL',\n",
    "                datum = 'MSL'\n",
    "            obs_times = []\n",
    "            waterLevel = []\n",
    "            waterLevelPreliminary = []\n",
    "            for row in csv.DictReader(opener.open(turl(id,datum))):\n",
    "                if row.has_key('Date Time'):\n",
    "                    if re.match('[12]\\d\\d\\d-\\d\\d-\\d\\d \\d\\d:\\d\\d',row['Date Time']):\n",
    "                        obs_times.append(dt.datetime.strptime(row['Date Time'], \"%Y-%m-%d %H:%M\"))\n",
    "                    else:\n",
    "                        pass# print row['Date Time']\n",
    "                else:\n",
    "                    print\n",
    "                    break\n",
    "                #print \"read\", row['Date Time']\n",
    "                if row.has_key(' Quality ') and row[' Water Level'] != '':\n",
    "                    if row[' Quality '] == 'v':\n",
    "                        waterLevel.append(row[' Water Level'])\n",
    "                        waterLevelPreliminary.append(None)\n",
    "                    else:\n",
    "                        # WaterLevel is preliminary, not verified\n",
    "                        waterLevel.append(None)\n",
    "                        waterLevelPreliminary.append(row[' Water Level'])\n",
    "                else:\n",
    "                    # print \"No Water Level\", row['Date Time']\n",
    "                    waterLevel.append(None)\n",
    "                    waterLevelPreliminary.append(None)\n",
    "            if len(obs_times) > 0:\n",
    "                print \"plotting\", station\n",
    "                fig,ax = plt.subplots(1,2,figsize=(16,4))\n",
    "                ax[0].set_ylabel(long_name+\"\\nfeet\")\n",
    "                ax[0].grid()\n",
    "                ax[0].plot(dt_time,y,label='ADCIRC')\n",
    "                ax[0].set_title(id)\n",
    "                ax[0].set_xlim(left=dt.datetime(2008,9,11,0),right=dt.datetime(2008,9,14,12))\n",
    "                ax[0].xaxis.set_major_formatter(dates.DateFormatter('%-m/%-d %-H'+'Z'))\n",
    "                ax[0].xaxis.set_major_locator(dates.HourLocator(byhour=[0,12]))\n",
    "                fig.autofmt_xdate()\n",
    "                ax[0].set_xlabel(base_date.strftime(\"%Y\"))\n",
    "                if any(waterLevel):\n",
    "                    ax[0].plot(obs_times,waterLevel,label='observed ('+datum+')')\n",
    "                if any(waterLevelPreliminary):\n",
    "                    ax[0].plot(obs_times,waterLevelPreliminary,label='observed ('+datum+')\\npreliminary',alpha=0.5)\n",
    "                ax[0].legend(loc='best')\n",
    "                map_points(ax[1],meshlon[i],meshlat[i])\n",
    "            else:\n",
    "                print \"no obs_times\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print thisstorm.basedir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare control to USGS Sensors (IKE only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False and thisstorm.name == 'IKE':\n",
    "    # USGS stations in Open-File Report 2008-1365\n",
    "    # Monitoring Inland Storm Surge and Flooding from Hurricane Ike in Texas and Louisiana, September 2008\n",
    "    # By Jeffery W. East, Michael J. Turco, and Robert R. Mason, Jr.\n",
    "    stations =[\n",
    "    [\"SSS-TX-BRA-001\", \"Brazoria\", 29.21194, -95.20833, \"surge\"],\n",
    "    [\"SSS-TX-BRA-002\", \"Brazoria\", 29.08472, -95.28806, \"surge\"],\n",
    "    [\"SSS-TX-BRA-004\", \"Brazoria\", 28.86833, -95.44861, \"surge\"],\n",
    "    [\"SSS-TX-BRA-005\", \"Brazoria\", 28.94944, -95.55556, \"riverine\"],\n",
    "    [\"SSS-TX-BRA-006\", \"Brazoria\", 28.86667, -95.58722, \"surge\"],\n",
    "    [\"SSS-TX-BRA-007\", \"Brazoria\", 29.28667, -95.13139, \"riverine\"],\n",
    "    [\"SSS-TX-BRA-008\", \"Brazoria\", 29.03556, -95.39889, \"surge\"],\n",
    "    [\"SSS-TX-BRA-009\", \"Brazoria\", 29.01306, -95.32972, \"surge\"],\n",
    "    [\"SSS-TX-BRA-010\", \"Brazoria\", 29.33639, -95.28417, \"riverine\"],\n",
    "    [\"SSS-TX-BRA-011\", \"Brazoria\", 29.29667, -95.35667, \"riverine\"],\n",
    "    [\"SSS-TX-CAL-001\", \"Calhoun\", 28.40639, -96.71167, \"surge\"],\n",
    "    [\"SSS-TX-CAL-002\", \"Calhoun\", 28.44444, -96.40250, \"surge\"],\n",
    "    [\"SSS-TX-CAL-003\", \"Calhoun\", 28.61917, -96.61972, \"surge\"],\n",
    "    [\"SSS-TX-CAL-004\", \"Calhoun\", 28.66056, -96.41167, \"surge\"],\n",
    "    [\"SSS-TX-CAL-005\", \"Calhoun\", 28.64139, -96.32333, \"surge\"],\n",
    "    [\"SSS-TX-CHA-003\", \"Chambers\", 29.60417, -94.67528, \"surge\"],\n",
    "    [\"SSS-TX-CHA-004\", \"Chambers\", 29.77278, -94.68694, \"surge\"],\n",
    "    [\"SSS-TX-GAL-001\", \"Galveston\", 29.45139, -94.63417, \"beach/wave\"],\n",
    "    [\"SSS-TX-GAL-002\", \"Galveston\", 29.46583, -94.64806, \"surge\"],\n",
    "    [\"SSS-TX-GAL-005\", \"Galveston\", 29.59444, -94.39028, \"surge\"],\n",
    "    [\"SSS-TX-GAL-008\", \"Galveston\", 29.33444, -94.75111, \"beach/wave\"],\n",
    "    [\"SSS-TX-GAL-010\", \"Galveston\", 29.23806, -94.87778, \"beach/wave\"],\n",
    "    [\"SSS-TX-GAL-011\", \"Galveston\", 29.22083, -94.94472, \"surge\"],\n",
    "    [\"SSS-TX-GAL-015\", \"Galveston\", 29.08611, -95.11722, \"beach/wave\"],\n",
    "    [\"SSS-TX-GAL-016\", \"Galveston\", 29.30389, -94.90528, \"surge\"],\n",
    "    [\"SSS-TX-GAL-018\", \"Galveston\", 29.35583, -95.04000, \"surge\"],\n",
    "    [\"SSS-TX-GAL-019\", \"Galveston\", 29.50639, -94.95778, \"surge\"],\n",
    "    [\"SSS-TX-GAL-020\", \"Galveston\", 29.45667, -95.04778, \"riverine\"],\n",
    "    [\"SSS-TX-GAL-021\", \"Galveston\", 29.51333, -95.10389, \"riverine\"],\n",
    "    [\"SSS-TX-GAL-022\", \"Galveston\", 29.55167, -95.02472, \"surge\"],\n",
    "    [\"SSS-TX-HAR-002\", \"Harris\", 29.62028, -94.99889, \"surge_sensor_psige\"],\n",
    "    [\"SSS-TX-HAR-003\", \"Harris\", 29.59194, -95.12833, \"surge\"],\n",
    "    [\"SSS-TX-HAR-004\", \"Harris\", 29.71306, -94.99333, \"surge\"],\n",
    "    [\"SSS-TX-JEF-001\", \"Jefferson\", 29.68444, -94.19278, \"surge\"],\n",
    "    [\"SSS-TX-JEF-002\", \"Jefferson\", 29.67500, -94.04361, \"beach/wave\"],\n",
    "    [\"SSS-TX-JEF-004\", \"Jefferson\", 29.71028, -94.11639, \"surge\"],\n",
    "    [\"SSS-TX-JEF-005\", \"Jefferson\", 29.69694, -94.09833, \"surge\"],\n",
    "    [\"SSS-TX-JEF-006\", \"Jefferson\", 29.71111, -93.86000, \"surge\"],\n",
    "    [\"SSS-TX-JEF-007\", \"Jefferson\", 29.77389, -93.94250, \"surge\"],\n",
    "    [\"SSS-TX-JEF-008\", \"Jefferson\", 29.76472, -93.89778, \"surge\"],\n",
    "    [\"SSS-TX-JEF-009\", \"Jefferson\", 29.66265, -94.08835, \"beach/wave\"],\n",
    "    [\"SSS-TX-MAT-001\", \"Matagorda\", 28.72056, -96.27389, \"surge\"],\n",
    "    [\"SSS-TX-MAT-002\", \"Matagorda\", 28.78639, -96.15028, \"surge\"],\n",
    "    [\"SSS-TX-MAT-003\", \"Matagorda\", 28.78750, -95.99583, \"riverine\"],\n",
    "    [\"SSS-TX-MAT-004\", \"Matagorda\", 28.83889, -95.85278, \"riverine\"],\n",
    "    [\"SSS-TX-MAT-005\", \"Matagorda\", 28.60056, -95.97806, \"beach/wave\"],\n",
    "    [\"SSS-TX-MAT-006\", \"Matagorda\", 28.68306, -95.97556, \"riverine\"],\n",
    "    [\"SSS-TX-MAT-007\", \"Matagorda\", 28.61139, -96.21528, \"surge\"],\n",
    "    [\"SSS-TX-MAT-008\", \"Matagorda\", 28.76417, -95.62694, \"beach/wave\"],\n",
    "    [\"SSS-TX-MAT-009\", \"Matagorda\", 28.77056, -95.61667, \"surge\"],\n",
    "    [\"SSS-TX-MAT-010\", \"Matagorda\", 28.83639, -95.66833, \"riverine\"],\n",
    "    [\"SSS-LA-CAM-001\", \"Cameron\", 29.75028, -93.66361, \"surge\"],\n",
    "    [\"SSS-LA-CAM-002\", \"Cameron\", 29.76194, -93.58250, \"surge\"],\n",
    "    [\"SSS-LA-CAM-003\", \"Cameron\", 29.80417, -93.34889, \"surge\"],\n",
    "    [\"SSS-LA-CAM-010\", \"Cameron\", 29.78611, -93.11500, \"surge\"],\n",
    "    [\"SSS-LA-CAM-011\", \"Cameron\", 29.87056, -93.07972, \"surge\"],\n",
    "    [\"SSS-LA-CAM-012\", \"Cameron\", 29.77056, -93.01444, \"surge\"],\n",
    "    [\"SSS-LA-VER-006\", \"Vermillion\", 29.64111, -92.42694, \"surge\"],\n",
    "    [\"SSS-LA-VER-007\", \"Vermillion\", 29.60028, -92.34167, \"surge\"]\n",
    "    ]\n",
    "\n",
    "\n",
    "    nc_file = thisstorm.basedir+'control/fort.63.nc'\n",
    "    print nc_file\n",
    "    fh = Dataset(nc_file, mode='r')\n",
    "    meshlat = fh.variables['y'][:]\n",
    "    meshlon = fh.variables['x'][:]\n",
    "    mtime   = fh.variables['time']\n",
    "    zeta    = fh.variables['zeta'][:]\n",
    "    base_date = dt.datetime.strptime(mtime.base_date, \"%Y-%m-%d %H:%M:%S %Z\")\n",
    "    dt_time = [base_date+dt.timedelta(0,t) for t in mtime]\n",
    "\n",
    "    units = fh.variables['zeta'].units\n",
    "    long_name = fh.variables['zeta'].long_name\n",
    "    fh.close()\n",
    "\n",
    "    # get scale factor for conversion to feet\n",
    "    newunits = 'feet'\n",
    "    oldunits = cf_units.Unit(units)\n",
    "    sf = oldunits.convert(1,cf_units.Unit(newunits))\n",
    "    units = newunits\n",
    "\n",
    "    for station in stations:\n",
    "        \n",
    "        site = station[0]\n",
    "        lat = station[2]\n",
    "        lon = station[3]\n",
    "        gtype = station[4]\n",
    "        # Find closest node in fort.63.nc\n",
    "        dlat = meshlat - lat\n",
    "        dlon = meshlon - lon\n",
    "        dist = np.sqrt(dlat**2+dlon**2)\n",
    "        i = np.argmin(dist)\n",
    "        y = zeta[:,i] * sf\n",
    "        if y.any():\n",
    "            fig,ax = plt.subplots(1,2,figsize=(16,4))\n",
    "            ax[0].set_ylabel(long_name+\"\\n\"+units)\n",
    "            ax[0].grid()\n",
    "            ax[0].plot(dt_time,y,label='ADCIRC')\n",
    "            ax[0].set_title(site+\"(\"+gtype+\")\")\n",
    "            ax[0].set_xlim(left=dt.datetime(2008,9,11,12),right=dt.datetime(2008,9,14,6))\n",
    "            ax[0].xaxis.set_major_formatter(dates.DateFormatter('%-m/%-d %-H'+'Z'))\n",
    "            ax[0].xaxis.set_major_locator(dates.HourLocator(byhour=[0,12]))\n",
    "            fig.autofmt_xdate()\n",
    "            ax[0].set_xlabel(base_date.strftime(\"%Y\"))\n",
    "\n",
    "            link = \"http://pubs.usgs.gov/of/2008/1365/downloads/ike_\"+site+\".txt\"\n",
    "            obs = np.genfromtxt(link,names=True,dtype=None,skip_header=28)\n",
    "            obs_times = []\n",
    "            elevations = []\n",
    "            for o in obs:\n",
    "                (date, time, elevation, surge_sensor_psi, nearest_barometric_sensor_psi, temp_from_surge_sensor,\n",
    "                temp_from_barometric_sensor, bad_psi_from_surge_sensor,bad_psi_from_barometric_sensor,\n",
    "                bad_temp_from_surge_sensor, bad_temp_from_barometric_sensor) = o\n",
    "                obs_times.append(dt.datetime.strptime(date+time, \"%m-%d-%Y%H:%M\")+dt.timedelta(hours=5)) # CDT time zone\n",
    "                elevations.append(elevation)\n",
    "            ax[0].plot(obs_times,elevations,label='observed')\n",
    "            ax[0].legend(loc='best')\n",
    "            map_points(ax[1],meshlon[i],meshlat[i], marker='o',scalebuffer=0.15,resolution='h')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Perturbations Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# thisstorm must be defined earlier.\n",
    "\n",
    "Ithresh = '1.00m'\n",
    "minus_astronomical_tide = False\n",
    "dryland = 'MHHW'\n",
    "\n",
    "if thisstorm.name == 'HARVEY' and minus_astronomical_tide:\n",
    "    print \"Not interested in stubtracting astronomical tide from Harvey\"\n",
    "    sys.exit(1)\n",
    "    \n",
    "\n",
    "#  veer perturbations\n",
    "veers = Perturbations(thisstorm, ptype='veers', units='km',xlabel='Distance from control at landfall', Ithresh=Ithresh,\n",
    "                     minus_astronomical_tide = minus_astronomical_tide, dryland=dryland)\n",
    "\n",
    "#  speed perturbations\n",
    "speeds = Perturbations(thisstorm, ptype='speeds',units='km',xlabel=\"Distance from control at landfall\",Ithresh=Ithresh,\n",
    "                      minus_astronomical_tide = minus_astronomical_tide, dryland=dryland)\n",
    "\n",
    "#  vmax perturbations\n",
    "vmaxes = Perturbations(thisstorm, ptype='vmaxes',units=\"kts\",xlabel = \"Intensity change at landfall\",Ithresh=Ithresh,\n",
    "                      minus_astronomical_tide = minus_astronomical_tide, dryland=dryland)\n",
    "\n",
    "#  rmax perturbations\n",
    "rmaxes = Perturbations(thisstorm, ptype='rmaxes',units=\"km\",xlabel=\"Difference in max. radius of 34-kt wind at landfall\",Ithresh=Ithresh,\n",
    "                      minus_astronomical_tide = minus_astronomical_tide, dryland=dryland)\n",
    "\n",
    "# IKE (could be put alongside CHARLEY)\n",
    "#IKEveers = Perturbations(IKE, ptype='veers', Ithresh = Ithresh)\n",
    "\n",
    "\n",
    "Perturbations_list = [veers, speeds, rmaxes, vmaxes]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series of Inundation Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oprefix(perturbations, ptype=False):\n",
    "    p_namestr = ''\n",
    "    if ptype == True:\n",
    "        p_namestr = '.' + perturbations.ptype\n",
    "    prefix =  perturbations.storm.basedir + perturbations.storm.name + \\\n",
    "    p_namestr + \\\n",
    "    '.minus_astronomical_tide' + str(minus_astronomical_tide) + \\\n",
    "    '.' + perturbations.Ithresh + '.' + perturbations.domain\n",
    "    return prefix\n",
    "\n",
    "fig, axs = plt.subplots(sharey=True,ncols=2,nrows=2,figsize=(10,7))\n",
    "\n",
    "for ax, perturbations in zip(axs.flatten(), Perturbations_list):\n",
    "    if len(perturbations) == 0:\n",
    "        print \"no values for \"+perturbations.ptype+\" perturbations\"\n",
    "        continue\n",
    "\n",
    "    ax.set_title(perturbations.ptype)\n",
    "    for pert in perturbations:\n",
    "        line, = ax.plot(pert.time,pert.inund,label=pert.valuelabel,markersize=3)\n",
    "        if pert.value == 0:\n",
    "            line.set_linewidth(3)\n",
    "            line.set_color('k')\n",
    "        if pert.value < 0:\n",
    "            line.set_marker('^')\n",
    "            line.set_markevery(3)\n",
    "\n",
    "\n",
    "    units = 'inundation volume ($\\mathregular{km^3}$)'\n",
    "    ax.set_ylabel(units)\n",
    "\n",
    "    ax.grid()\n",
    "\n",
    "    ax.xaxis.set_major_formatter(dates.DateFormatter('%-m/%-d %-H'+'Z'))\n",
    "    ax.xaxis.set_major_locator(dates.HourLocator(byhour=[0,6,12,18]))\n",
    "    fig.autofmt_xdate()\n",
    "    ax.set_xlabel(perturbations.time[0][0].strftime(\"%Y\"))\n",
    "    \n",
    "    # put legend\n",
    "    ax.legend(loc='best', ncol=2)\n",
    "\n",
    "plt.suptitle(thisstorm.name)\n",
    "\n",
    "status = mysavfig(oprefix(perturbations) + '_timeseries.png', string = perturbations.desc, **savfig_dict )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thisstorm.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum water level percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_bounds(domain):\n",
    "    #if stormname == 'CHARIKE': domain = 'stride02.-99.0E-87.0E25.0N31.0N'\n",
    "    #if stormname == 'IKE': domain = 'stride10.-99.0E-87.0E25.0N31.0N'\n",
    "    #if stormname == 'CHARLEY': domain = 'stride02.-85.0E-79.0E24.0N30.0N'\n",
    "    words =  domain.split('.')\n",
    "    stride = words[0]\n",
    "    lonmin = float(words[1]+'.'+words[2][0:1])\n",
    "    lonmax = float(words[2][2:]+'.'+words[3][0:1])\n",
    "    latmin = float(words[3][2:]+'.'+words[4][0:1])\n",
    "    latmax = float(words[4][2:]+'.'+words[5][0:1])\n",
    "    return lonmin, lonmax, latmin, latmax\n",
    "\n",
    "lonmin, lonmax, latmin, latmax = get_bounds(thisstorm.domain)\n",
    "\n",
    "\n",
    "# WARNING\n",
    "# THIS DOES NOT ISOLATE REGIONS LIKE FLORIDASW\n",
    "# IT STILL COVERS THE WHOLE LAT/LON BOX\n",
    "\n",
    "#  I SHOULD SWITCH TO perfect model file instead of looking for it here?\n",
    "# The perfect model file has the \"regional\" filter applied.\n",
    "\n",
    "# Don't assume control is called 'control'. It could be 'control_nws19'\n",
    "i0, = np.where(veers.values == 0)\n",
    "control_dir = veers.dirnames[i0[0]]\n",
    "fh = Dataset(control_dir + '/maxele.63.nc', mode='r')\n",
    "meshlon = fh.variables['x'][:]\n",
    "meshlat = fh.variables['y'][:]\n",
    "fh.close()\n",
    "ibox = (meshlon >= lonmin) & (meshlon <= lonmax) & (meshlat >= latmin) & (meshlat <= latmax)\n",
    "meshlon = meshlon[ibox]\n",
    "meshlat = meshlat[ibox]\n",
    "for perturbations in Perturbations_list:\n",
    "\n",
    "    if len(perturbations) == 0:\n",
    "        print \"no values for \"+perturbations.name+\" perturbations\"\n",
    "        continue\n",
    "    fig, ax = plt.subplots(1,2,figsize=(12,4))\n",
    "    ps = np.array([95,99,99.9,100])\n",
    "    for p in ps:\n",
    "        maxele = []\n",
    "        for pert in perturbations:\n",
    "            maxele.append(np.percentile(pert.maxele[ibox],p))\n",
    "        # Used to plot perturbation.values on x-axis. Changed Feb 28, 2017.\n",
    "        line, = ax[0].plot(perturbations.get_xticks(),maxele,marker='o',label='%.1f'%p)\n",
    "    annotate_pts(perturbations, line)\n",
    "        \n",
    "    imaxs = []\n",
    "    for pert in perturbations:\n",
    "        t = np.argmax(pert.maxele[ibox])\n",
    "        imaxs.append(t)\n",
    "                \n",
    "    ax[0].set_ylabel('max. water level (m)')\n",
    "    ax[0].set_title(\"Percentiles of Max. Water Level for \"+thisstorm.name+\"\\n\"+thisstorm.domain)\n",
    "    ax[0].yaxis.grid()\n",
    "    ax[0].set_ylim(0,12)\n",
    "    legend = ax[0].legend(loc='best', fontsize=12)\n",
    "    ax[0].set_xlabel(perturbations.xlabel)\n",
    "    zeroLine(line)\n",
    "    forecast_err_boxes(ax[0],perturbations)\n",
    "\n",
    "    m = map_points(ax[1], meshlon[imaxs], meshlat[imaxs], marker='o', colors=line.get_color(), \n",
    "                   labels=perturbations.valuelabels)\n",
    "    ax[1].set_title('Locations of Max. Water Level for\\n'+perturbations.xlabel)\n",
    "    \n",
    "    status = mysavfig(oprefix(perturbations,ptype=True)+ '_maxele.png',string=perturbations.desc, **savfig_dict)\n",
    "\n",
    "\n",
    "    \n",
    "# Why do I get \"double\" veer (track) lines,?\n",
    "\n",
    "# maybe you haven't symbolically linked \"control\"\n",
    "# directory to \"track+0\",  \"veer+0\", \"vmax+0\",  \"speed+0\", etc.\n",
    "# No. That did not fix it. I didn't negate distance errors for negative veers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://drive.google.com/open?id=1n58sZmEkW0J1YR9DRvasOQwQpp2Ojwvm0fI3VBQTk_k\">Google Doc about length scale</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(sharey=True,ncols=2,nrows=2,figsize=(10,7))\n",
    "\n",
    "for ax, perturbations in zip(axs.flatten(), Perturbations_list):\n",
    "    if len(perturbations) == 0:\n",
    "        print \"no values for \"+perturbations.name+\" perturbations\"\n",
    "        continue\n",
    "\n",
    "    thisline, = ax.plot(perturbations.get_xticks(),perturbations.length_scale,marker='o')\n",
    "    annotate_pts(perturbations, thisline)\n",
    "    ax.set_ylabel('length scale (km)')\n",
    "    ax.set_title(perturbations.ptype)\n",
    "    ax.yaxis.grid()\n",
    "    ax.set_xlabel(perturbations.xlabel)\n",
    "    zeroLine(thisline)\n",
    "    forecast_err_boxes(ax, perturbations)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(thisstorm.name)\n",
    "\n",
    "status = mysavfig(oprefix(perturbations)+'_length_scale.png',string=perturbations.desc, **savfig_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area of Inundation Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(sharey=True,ncols=2,nrows=2,figsize=(10,7))\n",
    "\n",
    "for ax, perturbations in zip(axs.flatten(), Perturbations_list):\n",
    "    if len(perturbations) == 0:\n",
    "        print \"no values for \"+perturbations.name+\" perturbations\"\n",
    "        continue\n",
    "\n",
    "    # use comma after thisline to \"un-list\" the one-item list of line objects\n",
    "    thisline, = ax.plot(perturbations.get_xticks(),perturbations.area,marker='o')\n",
    "    annotate_pts(perturbations, thisline)\n",
    "    ax.set_ylabel('area of inundation zone\\n$\\mathregular{km^2}$')\n",
    "    ax.set_title(perturbations.ptype)\n",
    "    ax.yaxis.grid()\n",
    "    ax.set_xlabel(perturbations.xlabel)\n",
    "    zeroLine(thisline)\n",
    "    forecast_err_boxes(ax, perturbations)\n",
    "\n",
    "plt.suptitle(thisstorm.name)\n",
    "plt.tight_layout()\n",
    "\n",
    "status = mysavfig(oprefix(perturbations)+'_area.png',string=perturbations.desc, **savfig_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Depth in Inundation Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(sharey=True,ncols=2,nrows=2,figsize=(10,7))\n",
    "\n",
    "for ax, perturbations in zip(axs.flatten(),Perturbations_list):\n",
    "    if len(perturbations) == 0:\n",
    "        print \"no values for \"+perturbations.name+\" perturbations\"\n",
    "        continue\n",
    "\n",
    "    thisline, = ax.plot(perturbations.get_xticks(),perturbations.depth,marker='o')\n",
    "    ax.set_ylabel('avg. depth in inundation zone\\nm')\n",
    "    annotate_pts(perturbations, thisline)\n",
    "    ax.set_title(perturbations.ptype)\n",
    "    ax.yaxis.grid()\n",
    "    ax.set_xlabel(perturbations.xlabel)\n",
    "    zeroLine(thisline)\n",
    "    forecast_err_boxes(ax, perturbations)\n",
    "\n",
    "plt.suptitle(thisstorm.name)\n",
    "plt.tight_layout()\n",
    "\n",
    "status = mysavfig(oprefix(perturbations)+'_depth.png',string=perturbations.desc, **savfig_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson correlation coefficient squared, Bias, Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for perturbations in Perturbations_list:\n",
    "    if len(perturbations) == 0:\n",
    "        print \"no values for \"+perturbations.name+\" perturbations\"\n",
    "        continue\n",
    "\n",
    "    r2 = []\n",
    "    bias = []\n",
    "    rmse = []\n",
    "    fig, ax = plt.subplots(figsize=(7,4.5))\n",
    "\n",
    "    for pdir in perturbations.dirnames:\n",
    "        # for consistency I started outputting all file names with stride, even\n",
    "        # the perfect_cntl.ncl script. That is always stride01, hence the\n",
    "        # change here.\n",
    "        search_domain = perturbations.domain.replace('stride02','stride01')\n",
    "        search_str = pdir + '/*minus_astronomical_tide' + str(minus_astronomical_tide) +\\\n",
    "            '_' + perturbations.Ithresh + '.' + dryland + '.' + search_domain +\\\n",
    "            '.perfectmodel.nc'\n",
    "        if len(glob.glob(search_str)) == 0:\n",
    "            print 'Found no files matching', search_str\n",
    "        for nc_file in glob.glob(search_str):\n",
    "\n",
    "            print 'reading '+nc_file,\n",
    "            fh = Dataset(nc_file, mode='r')\n",
    "            r2.append(fh.variables['r2'][:])\n",
    "            yave = np.mean(fh.variables['model'])\n",
    "            xave = np.mean(fh.variables['obs'])\n",
    "            bias.append(yave/xave)\n",
    "            obs = fh.variables['obs'][:] # tried np.asarray() but it removed _FillValue\n",
    "            model = fh.variables['model'][:]\n",
    "            rmse.append(np.sqrt(np.mean((obs - model)**2)))\n",
    "            fh.close()\n",
    "\n",
    "    print perturbations.values, r2\n",
    "    plt.title(perturbations.ptype)\n",
    "\n",
    "    r2p, = plt.plot(perturbations.get_xticks(),r2,label='$\\mathregular{r^2}$',marker='o')\n",
    "    # biasp, = plt.plot(perturbations.values,bias,label='multiplic. bias',marker='o')\n",
    "\n",
    "\n",
    "    ax.set_xlabel(perturbations.xlabel)\n",
    "    zeroLine(r2p)\n",
    "    forecast_err_boxes(ax,perturbations)\n",
    "\n",
    "    ax.yaxis.grid()\n",
    "    ax.set_ylim(0,1.2)\n",
    "    ax.set_ylabel('Correlation Coefficient ($\\mathregular{r^2}$)')\n",
    "    ax2 = ax.twinx()\n",
    "    # Fix the problem with the x-axis changing range\n",
    "    ax2.autoscale(False)\n",
    "    ax2.set_ylim(0,1.2)\n",
    "    ax2.set_ylabel('RMSE (m)')\n",
    "    rmsep, = ax2.plot(perturbations.get_xticks(),rmse,label='rmse',marker='o',color='red')\n",
    "    # Label line with perturbation magnitudes    \n",
    "    annotate_pts(perturbations, r2p)\n",
    "\n",
    "\n",
    "    # put legend to the right of the current axis\n",
    "    plt.legend(handles=[r2p,rmsep],loc='upper left', bbox_to_anchor=(1.1,1), fontsize=12)\n",
    "\n",
    "    status = mysavfig(oprefix(perturbations,ptype=True)+'_stats.png',bbox_inches='tight',\n",
    "                      string=perturbations.desc, **savfig_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storm following/Control zone Max. Inundation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(sharey=True,ncols=2,nrows=2,figsize=(10,7.))\n",
    "\n",
    "for ax, perturbations in zip(axs.flatten(), Perturbations_list):\n",
    "    if len(perturbations) == 0:\n",
    "        print \"no values for \"+perturbations.name+\" perturbations\"\n",
    "        continue\n",
    "\n",
    "    storm_following_line, = ax.plot(perturbations.get_xticks(),perturbations.max_vol, marker='o',label='storm-following')\n",
    "    fixed_location_line, = ax.plot(perturbations.get_xticks(),perturbations.max_vol_in_ctrl, marker='o',label='fixed location')\n",
    "    if perturbations.next().fixed_time: # is this okay? calling next() to get one element from generator?\n",
    "        fixed_location_line.set_label(fixed_location_line.get_label() + ', fixed time')\n",
    "    ax.set_ylabel('inundation volume ($\\mathregular{km^3}$)')\n",
    "    ax.set_title(perturbations.ptype)\n",
    "    ax.set_xlabel(perturbations.xlabel)\n",
    "    zeroLine(storm_following_line,alpha=0.4)\n",
    "    zeroLine(fixed_location_line,alpha=0.4)\n",
    "    forecast_err_boxes(ax, perturbations)\n",
    "    ax.yaxis.grid()\n",
    "\n",
    "    \n",
    "    # Of the 2 lines, label the one with greater mean values    \n",
    "    if np.mean(perturbations.max_vol) > np.mean(perturbations.max_vol_in_ctrl):\n",
    "        annotate_pts(perturbations, storm_following_line)\n",
    "    else:\n",
    "        annotate_pts(perturbations, fixed_location_line)\n",
    "\n",
    "# put legend to the right of the current axis\n",
    "ax.legend(loc='best')\n",
    "plt.suptitle(thisstorm.name)\n",
    "plt.tight_layout()\n",
    "\n",
    "status = mysavfig(oprefix(perturbations)+'_maxvol.png',string=perturbations.desc, **savfig_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variability at a single point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbations = veers\n",
    "fig, ax = plt.subplots(1,2, figsize=(14,5))\n",
    "colors=[]\n",
    "lons=[]\n",
    "lats=[]\n",
    "nodes = [i['index'] for i in perturbations.pointA]\n",
    "As, i = np.unique(nodes, return_index=True)\n",
    "# convert list to numpy array to index with a slice of indices (i)\n",
    "for pointA in np.array(perturbations.pointA)[i]:\n",
    "    A = pointA['index']\n",
    "    lon = pointA['lon']\n",
    "    lat = pointA['lat']\n",
    "    a = []\n",
    "    for mxfile in perturbations.mxfile:\n",
    "        fh = Dataset(mxfile, mode='r')\n",
    "        zeta_max = fh.variables['zeta_max']\n",
    "        a.append(zeta_max[A])\n",
    "        fh.close()\n",
    "\n",
    "    line, = ax[0].plot(perturbations.get_xticks(), a, marker='x', markeredgewidth=2, label=str(A)+' (%.2fN ' % lat + '%.2fE)' % lon)\n",
    "    zeroLine(line)\n",
    "\n",
    "    colors.append(line.get_color())\n",
    "    lats.append(lat)\n",
    "    lons.append(lon)\n",
    "m = map_points(ax[1], lons, lats, marker='x', colors=colors, linewidth=2, scalebuffer=0.5)\n",
    "\n",
    "\n",
    "#ax[0].legend(loc='best',fontsize=8)\n",
    "ax[0].set_title(thisstorm.name)\n",
    "ax[0].set_xlabel(perturbations.xlabel)\n",
    "forecast_err_boxes(ax[0], perturbations)\n",
    "ax[0].yaxis.grid()\n",
    "ax[0].set_ylim(0, pointA['thresh'])\n",
    "ax[0].set_ylabel('max. inundation depth (m)')\n",
    "\n",
    "tracks = '/glade/scratch/ahijevyc/ADCIRC/IKE/[cv]*[l05]/'\n",
    "if thisstorm.name == 'CHARLEY': tracks = '/glade/p/work/ahijevyc/ADCIRC/CHARLEY/[cv][oe]*/'\n",
    "for nc_file in glob.glob(tracks+'fort.22'):\n",
    "    lat0, lon0 = np.genfromtxt(nc_file,delimiter=',', usecols=(6,7),dtype=None, unpack=True)\n",
    "    lat0 = [int(i[:-1])/10. for i in lat0]\n",
    "    lon0 = [int(i[:-1])/ -10. for i in lon0]\n",
    "    m.plot(lon0, lat0, 'black', latlon=True, label=nc_file[70:74],linewidth=(3 if 'control' in nc_file else .71))\n",
    "\n",
    "\n",
    "\n",
    "status = mysavfig(oprefix(perturbations,ptype=True)+'_pointA_maxvol.png',string=perturbations.desc, **savfig_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
